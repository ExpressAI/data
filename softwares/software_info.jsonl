{"ad_from_product_description---default": {"description": "This promptware is used to turn a product description into ad copy.", "creator": "OpenAI", "homepage": "https://beta.openai.com/examples/", "reference": "", "codebase_url": "https://beta.openai.com/examples/", "license": "no-license", "task": null, "research_tasks": ["conditional-generation"], "application_categories": ["generation"], "application_subcategories": ["text-generation"], "original_platform": "gpt3", "design_pattern": "standalone", "source_language": "en", "target_language": "en", "module_name": "ad_from_product_description_promptware", "config_name": "default", "kernel_configs": {"openai": {"platform": "openai", "model_name": "text-curie-001", "max_tokens": 100, "temperature": 0.5, "top_p": 1.0, "suffix": "", "frequency_penalty": 0.0, "presence_penalty": 0.0, "logprobs": 1, "n": 1, "echo": false, "stop": null}}, "software_configs": {"ad_from_product_description": {"name": "ad_from_product_description", "description": "This promptware is used to turn a product description into ad copy.", "instruction": "Write a creative ad for the following product to run on Facebook aimed at parents:\n\n", "demonstration": [], "prompt_template": "lambda input: f\"{input['text']}\",", "task": "conditional-generation"}}, "dataset_configs": null, "example": {"input": {"text": "Product: Learning Room is a virtual environment to help students from kindergarten to high school excel in school."}, "output": "Learning Room is the perfect virtual environment for parents to help their children excel in school. With Learning Room, parents can help their children learn at their own pace, in a safe and secure environment."}, "example_prompt": "Write a creative ad for the following product to run on Facebook aimed at parents:\n\n\n\nProduct: Learning Room is a virtual environment to help students from kindergarten to high school excel in school.\n"}}
{"analogy_maker---default": {"description": "This promptware is used to create analogies. Modified from a community prompt to require fewer examples.", "creator": "OpenAI", "homepage": "https://beta.openai.com/examples/", "reference": "", "codebase_url": "https://beta.openai.com/examples/", "license": "no-license", "task": null, "research_tasks": ["conditional-generation"], "application_categories": ["generation"], "application_subcategories": ["text-generation"], "original_platform": "gpt3", "design_pattern": "standalone", "source_language": "en", "target_language": "en", "module_name": "analogy_maker_promptware", "config_name": "default", "kernel_configs": {"openai": {"platform": "openai", "model_name": "text-davinci-003", "max_tokens": 60, "temperature": 0.5, "top_p": 1.0, "suffix": "", "frequency_penalty": 0.0, "presence_penalty": 0.0, "logprobs": 1, "n": 1, "echo": false, "stop": null}}, "software_configs": {"analogy_maker": {"name": "analogy_maker", "description": "This promptware is used to create analogies. Modified from a community prompt to require fewer examples.", "instruction": "Create an analogy for this phrase:\n\n", "demonstration": [], "prompt_template": "lambda input: f\"{input['text']}\",", "task": "conditional-generation"}}, "dataset_configs": null, "example": {"input": {"text": "Questions are arrows in that:"}, "output": "Questions are like arrows in that they both have the potential to hit their target. Just as an arrow needs to be aimed correctly to hit its mark, a question needs to be asked in the right way to get the desired response."}, "example_prompt": "Create an analogy for this phrase:\n\n\n\nQuestions are arrows in that:\n"}}
{"antonyms---default": {"description": "This promptware is used to identify the relationship between two words", "creator": "Promptware Authors", "homepage": "https://github.com/expressai/promptware", "reference": "", "codebase_url": "https://github.com/expressai/promptware/tree/main/softwares", "license": "apache-2.0", "task": null, "research_tasks": ["antonym-identification"], "application_categories": ["generation"], "application_subcategories": ["text-generation"], "original_platform": "gpt3", "design_pattern": "standalone", "source_language": "en", "target_language": "en", "module_name": "antonyms_promptware", "config_name": "default", "kernel_configs": {"openai": {"platform": "openai", "model_name": "text-curie-001", "max_tokens": 64, "temperature": 0, "top_p": 1, "suffix": "", "frequency_penalty": 0.0, "presence_penalty": 0.0, "logprobs": 1, "n": 1, "echo": false, "stop": null}}, "software_configs": {"antonyms": {"name": "antonyms", "description": "This promptware is used to identify the  relationship between two words", "instruction": "take the inputted word and pair it with its antonym", "demonstration": ["Informed\nUninformed\n\n.", "shout\nwhisper\n\n", "harmony\ndissonance\n\n"], "prompt_template": "lambda input: f\"{input['text']}\",", "task": ["conditional-generation"]}}, "dataset_configs": null, "example": {"input": {"text": "correct"}, "output": "incorrect"}, "example_prompt": "take the inputted word and pair it with its antonym\nInformed\nUninformed\n\n.\nshout\nwhisper\n\n\nharmony\ndissonance\n\n\ncorrect\n"}}
{"calculate_time_complexity---default": {"description": "This promptware is used to find the time complexity of a function.", "creator": "OpenAI", "homepage": "https://beta.openai.com/examples/", "reference": "", "codebase_url": "https://beta.openai.com/examples/", "license": "no-license", "task": null, "research_tasks": ["conditional-generation"], "application_categories": ["others"], "application_subcategories": ["others"], "original_platform": "gpt3", "design_pattern": "standalone", "source_language": "en", "target_language": "en", "module_name": "calculate_time_complexity_promptware", "config_name": "default", "kernel_configs": {"openai": {"platform": "openai", "model_name": "text-davinci-003", "max_tokens": 64, "temperature": 0, "top_p": 1.0, "suffix": "", "frequency_penalty": 0.0, "presence_penalty": 0.0, "logprobs": 1, "n": 1, "echo": false, "stop": ["\n"]}}, "software_configs": {"calculate_time_complexity": {"name": "calculate_time_complexity", "description": "This promptware is used to find the time complexity of a function.", "instruction": "", "demonstration": [], "prompt_template": "lambda input: f\"{input['text']}\",", "task": "conditional-generation"}}, "dataset_configs": null, "example": {"input": {"text": "def foo(n, k):\naccum = 0\nfor i in range(n):\n    for l in range(k):\n        accum += i\nreturn accum\n\"\"\"\nThe time complexity of this function is"}, "output": "O(n*k)."}, "example_prompt": "\ndef foo(n, k):\naccum = 0\nfor i in range(n):\n    for l in range(k):\n        accum += i\nreturn accum\n\"\"\"\nThe time complexity of this function is\n"}}
{"conversation---default": {"description": "Open ended conversation with an AI assistant.", "creator": "OpenAI", "homepage": "https://beta.openai.com/examples/", "reference": "", "codebase_url": "https://beta.openai.com/examples/", "license": "no-license", "task": null, "research_tasks": ["qa-open-domain"], "application_categories": ["conversation"], "application_subcategories": ["open-ended-conversation"], "original_platform": "gpt3", "design_pattern": "standalone", "source_language": "en", "target_language": "en", "module_name": "conversation_promptware", "config_name": "default", "kernel_configs": {"openai1": {"platform": "openai", "model_name": "text-curie-001", "max_tokens": 150, "temperature": 0.9, "top_p": 1, "suffix": "", "frequency_penalty": 0.0, "presence_penalty": 0.6, "logprobs": 1, "n": 1, "echo": false, "stop": null}, "openai2": {"platform": "openai", "model_name": "text-curie-001", "max_tokens": 60, "temperature": 0.5, "top_p": 1, "suffix": "", "frequency_penalty": 0.5, "presence_penalty": 0, "logprobs": 1, "n": 1, "echo": false, "stop": null}, "openai3": {"platform": "openai", "model_name": "text-curie-001", "max_tokens": 60, "temperature": 0.5, "top_p": 0.5, "suffix": "", "frequency_penalty": 0.5, "presence_penalty": 0, "logprobs": 1, "n": 1, "echo": false, "stop": null}}, "software_configs": {"conversation_general": {"name": "conversation_general", "description": "Open ended conversation with an AI assistant.", "instruction": "The following is a conversation with an AI assistant. The assistant is helpful, creative, clever, and very friendly.", "demonstration": ["Human: Hello, who are you?\nAI: I am an AI created by OpenAI. How can I help you today?\n"], "prompt_template": "lambda input: f\"Human: {input['text']}\\nAI:\",", "task": "qa-open-domain"}}, "dataset_configs": null, "example": {"input": {"text": "I'd like to cancel my subscription."}, "output": "Certainly, cancel your subscription anytime you like. We hope you enjoy using our services!"}, "example_prompt": "The following is a conversation with an AI assistant. The assistant is helpful, creative, clever, and very friendly.\nHuman: Hello, who are you?\nAI: I am an AI created by OpenAI. How can I help you today?\n\nHuman: I'd like to cancel my subscription.\nAI:\n"}}
{"create_study_notes---default": {"description": "This promptware is used to provide a topic and get study notes.", "creator": "OpenAI", "homepage": "https://beta.openai.com/examples/", "reference": "", "codebase_url": "https://beta.openai.com/examples/", "license": "no-license", "task": null, "research_tasks": ["conditional-generation"], "application_categories": ["generation"], "application_subcategories": ["text-generation"], "original_platform": "gpt3", "design_pattern": "standalone", "source_language": "en", "target_language": "en", "module_name": "create_study_notes_promptware", "config_name": "default", "kernel_configs": {"openai": {"platform": "openai", "model_name": "text-davinci-003", "max_tokens": 150, "temperature": 0.3, "top_p": 1.0, "suffix": "", "frequency_penalty": 0.0, "presence_penalty": 0.0, "logprobs": 1, "n": 1, "echo": false, "stop": null}}, "software_configs": {"create_study_notes": {"name": "create_study_notes", "description": "This promptware is used to provide a topic and get study notes.", "instruction": "", "demonstration": [], "prompt_template": "lambda input: f\"{input['text']}\",", "task": "conditional-generation"}}, "dataset_configs": null, "example": {"input": {"text": "What are 5 key points I should know when studying Ancient Rome?"}, "output": "1. The Roman Republic: This was the period of Ancient Rome from 509 BC to 27 BC, when it was ruled by a senate and two consuls.\n\n2. The Roman Empire: This was the period of Ancient Rome from 27 BC to 476 AD, when it was ruled by an emperor.\n\n3. Roman Law: This was the system of laws that governed the Roman Republic and Empire. It was based on the Twelve Tables, a set of laws written in the 5th century BC.\n\n4. Roman Religion: This was the religion of Ancient Rome, which was polytheistic and focused on the worship of gods and goddesses.\n\n5. Roman Architecture: This was the style of architecture"}, "example_prompt": "\nWhat are 5 key points I should know when studying Ancient Rome?\n"}}
{"esrb_rating---default": {"description": "This promptware is used to categorize text based upon ESRB ratings.", "creator": "OpenAI", "homepage": "https://beta.openai.com/examples/", "reference": "", "codebase_url": "https://beta.openai.com/examples/", "license": "no-license", "task": null, "research_tasks": ["text-classification"], "application_categories": ["classification"], "application_subcategories": ["general-classification"], "original_platform": "gpt3", "design_pattern": "standalone", "source_language": "en", "target_language": "en", "module_name": "esrb_ratings_promptware", "config_name": "default", "kernel_configs": {"openai": {"platform": "openai", "model_name": "text-davinci-003", "max_tokens": 60, "temperature": 0, "top_p": 1.0, "suffix": "", "frequency_penalty": 0.0, "presence_penalty": 0.0, "logprobs": 1, "n": 1, "echo": false, "stop": ["\n"]}}, "software_configs": {"esrb_rating": {"name": "esrb_rating", "description": "This promptware is used to categorize text based upon ESRB ratings.", "instruction": "Provide an ESRB rating for the following text:\n\n", "demonstration": [], "prompt_template": "lambda input: f\"{input['text']}\",", "task": "conditional-generation"}}, "dataset_configs": null, "example": {"input": {"text": "\"i'm going to blow your brains out with my ray gun then stomp on your guts.\"\n\nESRB rating:"}, "output": "Mature (M)"}, "example_prompt": "Provide an ESRB rating for the following text:\n\n\n\n\"i'm going to blow your brains out with my ray gun then stomp on your guts.\"\n\nESRB rating:\n"}}
{"essay_outline---default": {"description": "This promptware is used to generate an outline for a research topic.", "creator": "OpenAI", "homepage": "https://beta.openai.com/examples/", "reference": "", "codebase_url": "https://beta.openai.com/examples/", "license": "no-license", "task": null, "research_tasks": ["conditional-generation"], "application_categories": ["generation"], "application_subcategories": ["text-generation"], "original_platform": "gpt3", "design_pattern": "standalone", "source_language": "en", "target_language": "en", "module_name": "essay_outline_promptware", "config_name": "default", "kernel_configs": {"openai": {"platform": "openai", "model_name": "text-davinci-003", "max_tokens": 150, "temperature": 0.3, "top_p": 1.0, "suffix": "", "frequency_penalty": 0.0, "presence_penalty": 0.0, "logprobs": 1, "n": 1, "echo": false, "stop": null}}, "software_configs": {"essay_outline": {"name": "essay_outline", "description": "This promptware is used to generate an outline for a research topic.", "instruction": "", "demonstration": [], "prompt_template": "lambda input: f\"{input['text']}\",", "task": "conditional-generation"}}, "dataset_configs": null, "example": {"input": {"text": "Create an outline for an essay about Nikola Tesla and his contributions to technology:"}, "output": "I. Introduction\nA. Definition of Nikola Tesla\nB. Overview of Tesla's life and accomplishments\n\nII. Early Life and Education\nA. Tesla's birthplace and family\nB. Tesla's education and early career\n\nIII. Contributions to Technology\nA. Alternating current\nB. Radio and remote control\nC. Robotics and artificial intelligence\nD. X-rays and lasers\n\nIV. Legacy of Tesla\nA. Impact on modern technology\nB. Recognition of Tesla's work\n\n V. Conclusion\nA. Summary of Tesla's life and accomplishments\nB. Reflection on Tesla's legacy"}, "example_prompt": "\nCreate an outline for an essay about Nikola Tesla and his contributions to technology:\n"}}
{"explain_code---default": {"description": "Explain a complicated piece of code.", "creator": "OpenAI", "homepage": "https://beta.openai.com/examples/default-explain-code", "reference": "", "codebase_url": "https://beta.openai.com/examples/default-explain-code", "license": "no-license", "task": null, "research_tasks": ["conditional-generation"], "application_categories": ["transformation"], "application_subcategories": ["explanation"], "original_platform": "gpt3", "design_pattern": "standalone", "source_language": "python", "target_language": "en", "module_name": "explain_code_promptware", "config_name": "default", "kernel_configs": {"openai1": {"platform": "openai", "model_name": "text-curie-001", "max_tokens": 64, "temperature": 0, "top_p": 1, "suffix": "", "frequency_penalty": 0.0, "presence_penalty": 0.0, "logprobs": 1, "n": 1, "echo": false, "stop": null}, "openai2": {"platform": "openai", "model_name": "text-curie-001", "max_tokens": 64, "temperature": 0, "top_p": 1, "suffix": "", "frequency_penalty": 0.0, "presence_penalty": 0.0, "logprobs": 1, "n": 1, "echo": false, "stop": null}}, "software_configs": {"explain_code_general": {"name": "explain_code_general", "description": "Explain a complicated piece of code.", "instruction": "A table summarizing the following text.", "demonstration": [], "prompt_template": "lambda input: f\"{input['code']}\\n\\\"\\\"\\\"\\n\"", "task": "conditional-generation"}}, "dataset_configs": null, "example": {"input": {"code": "class Log:\n                def __init__(self, path):\n                    dirname = os.path.dirname(path)\n                    os.makedirs(dirname, exist_ok=True)\n                    f = open(path, \"a+\")\n                    # Check that the file is newline-terminated\n                    size = os.path.getsize(path)\n                    if size > 0:\n                        f.seek(size - 1)\n                        end = f.read(1)\n                        if end != \"\n\":\n                            f.write(\"\n\")\n                    self.f = f\n                    self.path = path\n                def log(self, event):\n                    event[\"_event_id\"] = str(uuid.uuid4())\n                    json.dump(event, self.f)\n                    self.f.write(\"\n\")\n                def state(self):\n                    state = {\"complete\": set(), \"last\": None}\n                    for line in open(self.path):\n                        event = json.loads(line)\n                        if event[\"type\"] == \"submit\" and event[\"success\"]:\n                            state[\"complete\"].add(event[\"id\"])\n                            state[\"last\"] = event\n                    return state"}, "output": "The class Log is defined, and its __init__() method is called.\n2.\nThe path to the file to be logged is determined.\n3.\nThe file is opened for writing.\n4.\nEach line of the file is read and parsed into an event object.\n5."}, "example_prompt": "A table summarizing the following text.\n\nclass Log:\n                def __init__(self, path):\n                    dirname = os.path.dirname(path)\n                    os.makedirs(dirname, exist_ok=True)\n                    f = open(path, \"a+\")\n                    # Check that the file is newline-terminated\n                    size = os.path.getsize(path)\n                    if size > 0:\n                        f.seek(size - 1)\n                        end = f.read(1)\n                        if end != \"\n\":\n                            f.write(\"\n\")\n                    self.f = f\n                    self.path = path\n                def log(self, event):\n                    event[\"_event_id\"] = str(uuid.uuid4())\n                    json.dump(event, self.f)\n                    self.f.write(\"\n\")\n                def state(self):\n                    state = {\"complete\": set(), \"last\": None}\n                    for line in open(self.path):\n                        event = json.loads(line)\n                        if event[\"type\"] == \"submit\" and event[\"success\"]:\n                            state[\"complete\"].add(event[\"id\"])\n                            state[\"last\"] = event\n                    return state\n\"\"\"\nHere's what the above code is doing:\n1.\n"}}
{"extract_contact_information---default": {"description": "This promptware is used to extract contact information from a block of text.", "creator": "OpenAI", "homepage": "https://beta.openai.com/examples/", "reference": "", "codebase_url": "https://beta.openai.com/examples/", "license": "no-license", "task": null, "research_tasks": ["conditional-generation"], "application_categories": ["transformation"], "application_subcategories": ["extraction"], "original_platform": "gpt3", "design_pattern": "standalone", "source_language": "en", "target_language": "en", "module_name": "extract_contact_information_promptware", "config_name": "default", "kernel_configs": {"openai": {"platform": "openai", "model_name": "text-davinci-003", "max_tokens": 64, "temperature": 0, "top_p": 1.0, "suffix": "", "frequency_penalty": 0.0, "presence_penalty": 0.0, "logprobs": 1, "n": 1, "echo": false, "stop": null}}, "software_configs": {"extract_contact_information": {"name": "extract_contact_information", "description": "This promptware is used to extract contact information from a block of text.", "instruction": "Extract the name and mailing address from this email:\n\n", "demonstration": [], "prompt_template": "lambda input: f\"{input['text']}\",", "task": "conditional-generation"}}, "dataset_configs": null, "example": {"input": {"text": "Dear Kelly,\n\nIt was great to talk to you at the seminar. I thought Jane's talk was quite good.\n\nThank you for the book. Here's my address 2111 Ash Lane, Crestview CA 92002\n\nBest,\n\nMaya\n\nName:"}, "output": "Maya\nMailing Address:\n2111 Ash Lane, Crestview CA 92002"}, "example_prompt": "Extract the name and mailing address from this email:\n\n\n\nDear Kelly,\n\nIt was great to talk to you at the seminar. I thought Jane's talk was quite good.\n\nThank you for the book. Here's my address 2111 Ash Lane, Crestview CA 92002\n\nBest,\n\nMaya\n\nName:\n"}}
{"gaokao_cloze_hint---default": {"description": "Gaokao English Cloze Hint", "creator": "Promptware Authors", "homepage": "https://github.com/expressai/promptware", "reference": "", "codebase_url": "https://github.com/expressai/promptware/tree/main/softwares", "license": "apache-2.0", "task": null, "research_tasks": ["cloze-generative"], "application_categories": ["generation"], "application_subcategories": ["text-generation"], "original_platform": "gpt3", "design_pattern": "standalone", "source_language": "en", "target_language": "en", "module_name": "gaokao_cloze_hint_promptware", "config_name": "default", "kernel_configs": {"openai": {"platform": "openai", "model_name": "text-davinci-002", "max_tokens": 40, "temperature": 0.0, "top_p": 1, "suffix": "", "frequency_penalty": 0.0, "presence_penalty": 0.0, "logprobs": 1, "n": 1, "echo": false, "stop": null}}, "software_configs": {"gaokao_cloze_hint": {"name": "gaokao_cloze_hint", "description": "Gaokao English Cloze Hint", "instruction": "", "demonstration": ["There has been a recent trend in the food service industry toward lower fat content and less salt. This trend, which was started by the medical community <Q61> a method of fighting heart disease, has had some unintended side <Q62> such as overweight and heart disease the very thing the medical community was trying to fight. Fat and salt are very important parts of a diet. They are required <Q63> the food that we eat, to recover from injury and for several other bodily functions. When fat and salt <Q64> from food, the food tastes as if is missing something. As <Q65> result, people will eat more food to try to make up for that something missing. Even <Q66>, the amount of fast food that people eat goes up. Fast food <Q67> full of fat and salt; by <Q68> more fast food people will get more salt and fat than they need in their diet. Having enough fat and salt in your meals will reduce the urge to snack between meals and will improve the taste of your food. However, be <Q69> not to go to extremes. Like anything, it is possible to have too much of both, <Q70> is not good for the health.\nWhat should be filled in at the <Q61> position?\nas\n", "There has been a recent trend in the food service industry toward lower fat content and less salt. This trend, which was started by the medical community as a method of fighting heart disease, has had some unintended side effects such as overweight and heart disease the very thing the medical community was trying to fight. Fat and salt are very important parts of a diet. They are required to process the food that we eat, to recover from injury and for several other bodily functions. When fat and salt <Q64> from food, the food tastes as if is missing something. As <Q65> result, people will eat more food to try to make up for that something missing. Even <Q66>, the amount of fast food that people eat goes up. Fast food <Q67> full of fat and salt; by <Q68> more fast food people will get more salt and fat than they need in their diet. Having enough fat and salt in your meals will reduce the urge to snack between meals and will improve the taste of your food. However, be <Q69> not to go to extremes. Like anything, it is possible to have too much of both, <Q70> is not good for the health.\nWhat should be filled in at the <Q64> position given the hint \"remove\"?\nare removed"], "prompt_template": "lambda input: f\"{normalize_text(input['context'])}\\n\"", "task": "cloze-generative"}}, "dataset_configs": null, "example": {"input": {"context": "According to a review of evidence in a medical journal, runners live three years <Q61> than non-runners. You don't have to run fast or for long <Q62> the benefit. You may drink, smoke, be overweight and still reduce your risk of <Q63> early by running.\n\nWhile running regularly can't make you live forever, the review says it <Q64> more effective at lengthening life <Q65> walking, cycling or swimming. Two of the authors of the review also made a study published in 2014 <Q66> showed a mere five to 10 minutes a day of running reduced the risk of heart disease and early deaths from all <Q67>.\nThe best exercise is one that you enjoy and will do. But otherwise it's probably running. To avoid knee pain, you can run on soft surfaces, do exercises to <Q68> your leg muscles, avoid hills and get good running shoes. Running is cheap, easy and it's always <Q69>. If you are time poor, you need run for only half the time to get the same benefits as other sports, so perhaps we should all give <Q70> a try.", "hint": "long", "question_mark": "<Q61>", "answers": ["longer"]}, "output": "longer"}, "example_prompt": "There has been a recent trend in the food service industry toward lower fat content and less salt. This trend, which was started by the medical community <Q61> a method of fighting heart disease, has had some unintended side <Q62> such as overweight and heart disease the very thing the medical community was trying to fight. Fat and salt are very important parts of a diet. They are required <Q63> the food that we eat, to recover from injury and for several other bodily functions. When fat and salt <Q64> from food, the food tastes as if is missing something. As <Q65> result, people will eat more food to try to make up for that something missing. Even <Q66>, the amount of fast food that people eat goes up. Fast food <Q67> full of fat and salt; by <Q68> more fast food people will get more salt and fat than they need in their diet. Having enough fat and salt in your meals will reduce the urge to snack between meals and will improve the taste of your food. However, be <Q69> not to go to extremes. Like anything, it is possible to have too much of both, <Q70> is not good for the health.\nWhat should be filled in at the <Q61> position?\nas\n\nThere has been a recent trend in the food service industry toward lower fat content and less salt. This trend, which was started by the medical community as a method of fighting heart disease, has had some unintended side effects such as overweight and heart disease the very thing the medical community was trying to fight. Fat and salt are very important parts of a diet. They are required to process the food that we eat, to recover from injury and for several other bodily functions. When fat and salt <Q64> from food, the food tastes as if is missing something. As <Q65> result, people will eat more food to try to make up for that something missing. Even <Q66>, the amount of fast food that people eat goes up. Fast food <Q67> full of fat and salt; by <Q68> more fast food people will get more salt and fat than they need in their diet. Having enough fat and salt in your meals will reduce the urge to snack between meals and will improve the taste of your food. However, be <Q69> not to go to extremes. Like anything, it is possible to have too much of both, <Q70> is not good for the health.\nWhat should be filled in at the <Q64> position given the hint \"remove\"?\nare removed\nAccording to a review of evidence in a medical journal, runners live three years <Q61> than non-runners. You don't have to run fast or for long <Q62> the benefit. You may drink, smoke, be overweight and still reduce your risk of <Q63> early by running. While running regularly can't make you live forever, the review says it <Q64> more effective at lengthening life <Q65> walking, cycling or swimming. Two of the authors of the review also made a study published in 2014 <Q66> showed a mere five to 10 minutes a day of running reduced the risk of heart disease and early deaths from all <Q67>. The best exercise is one that you enjoy and will do. But otherwise it's probably running. To avoid knee pain, you can run on soft surfaces, do exercises to <Q68> your leg muscles, avoid hills and get good running shoes. Running is cheap, easy and it's always <Q69>. If you are time poor, you need run for only half the time to get the same benefits as other sports, so perhaps we should all give <Q70> a try.\nWhat should be filled in at the <Q61> position? given the hint \"long\"?\n\n"}}
{"gaokao_cloze_multiple_choice---default": {"description": "gaokao_cloze_multiple_choice", "creator": "Promptware Authors", "homepage": "https://github.com/expressai/promptware", "reference": "", "codebase_url": "https://github.com/expressai/promptware/tree/main/softwares", "license": "apache-2.0", "task": null, "research_tasks": ["cloze-multiple-choice"], "application_categories": ["conversation"], "application_subcategories": ["question-answering"], "original_platform": "gpt3", "design_pattern": "standalone", "source_language": "en", "target_language": "en", "module_name": "gaokao_cloze_multiple_choice_promptware", "config_name": "default", "kernel_configs": {"openai": {"platform": "openai", "model_name": "text-davinci-002", "max_tokens": 40, "temperature": 0.0, "top_p": 1, "suffix": "", "frequency_penalty": 0.0, "presence_penalty": 0.0, "logprobs": 1, "n": 1, "echo": false, "stop": null}}, "software_configs": {"gaokao_cloze_multiple_choice": {"name": "gaokao_cloze_multiple_choice", "description": "Gaokao English Cloze Multiple Choice", "instruction": "", "demonstration": ["If anyone had told me three years ago that I would be spending most of my weekends camping. I would have laughed heartily. Campers, in my eyes, were people who enjoyed insects bites, ill-cooked meals, and uncomfortable sleeping bags. They had nothing in common with me. <Q36> The friends who introduced me to camping thought that it meant to be a pioneer. <Q37> We sleep in a tent, cooked over an open fire, and walked a long distance to take the shower and use the bathroom. This brief visit with Mother Nature cost me two days off from work, recovering from a bad case of sunburn and the doctor's bill for my son's food poisoning. I was, nevertheless, talked into going on another fun-filled holiday in the wilderness. <Q38> Instead, we had a pop-up camper with comfortable beds and an air conditioner. My nature-loving friends had remembered to bring all the necessities of life. <Q39> We have done a lot of it since. Recently, we bought a twenty-eight-foot travel trailer complete with a bathroom and a built-in TV set. There is a separate bedroom, a modern kitchen with a refrigerator. The trailer even has matching carpet and curtains. <Q40> It must be true that sooner or later, everyone finds his or her way back to nature. I recommend that you find your way in style. \nWhat should be filled in at the <Q36> position? \"This time there was no tent.\", \"Things are going to be improved.\", \"The trip they took me on was a rough one.\", \"I was to learn a lot about camping since then, however.\", \"I must say that I have certainly come to enjoy camping.\", \"After the trip, my family became quite interested in camping.\" or \"There was no shade as the trees were no more than 3 feet tall.\"?\nI was to learn a lot about camping since then, however.\n", "If anyone had told me three years ago that I would be spending most of my weekends camping. I would have laughed heartily. Campers, in my eyes, were people who enjoyed insects bites, ill-cooked meals, and uncomfortable sleeping bags. They had nothing in common with me. I was to learn a lot about camping since then, however. The friends who introduced me to camping thought that it meant to be a pioneer. The trip they took me on was a rough one. We sleep in a tent, cooked over an open fire, and walked a long distance to take the shower and use the bathroom. This brief visit with Mother Nature cost me two days off from work, recovering from a bad case of sunburn and the doctor's bill for my son's food poisoning. I was, nevertheless, talked into going on another fun-filled holiday in the wilderness. <Q38> Instead, we had a pop-up camper with comfortable beds and an air conditioner. My nature-loving friends had remembered to bring all the necessities of life. <Q39> We have done a lot of it since. Recently, we bought a twenty-eight-foot travel trailer complete with a bathroom and a built-in TV set. There is a separate bedroom, a modern kitchen with a refrigerator. The trailer even has matching carpet and curtains. <Q40> It must be true that sooner or later, everyone finds his or her way back to nature. I recommend that you find your way in style. \nWhat should be filled in at the <Q38> position? \"This time there was no tent.\", \"Things are going to be improved.\", \"The trip they took me on was a rough one.\", \"I was to learn a lot about camping since then, however.\", \"I must say that I have certainly come to enjoy camping.\", \"After the trip, my family became quite interested in camping.\" or \"There was no shade as the trees were no more than 3 feet tall.\"?\nThis time there was no tent.\n"], "prompt_template": "lambda input: f\"{normalize_text(input['context'])}\\n\"", "task": "cloze-multiple-choice"}}, "dataset_configs": null, "example": {"input": {"context": "Color is fundamental in home design \u2014 something you'll always have in every room. A grasp of how to manage color in your spaces is one of the first steps to creating rooms you'll love to live in. Do you want a room that's full of life? Professional? Or are you just looking for a place to relax after a long day? <Q36>, color is the key to making a room feel the way you want it to feel.\n\nOver the years, there have been a number of different techniques to help designers approach this important point. <Q37>, they can get a little complex. But good news is that there're really only three kinds of decisions you need to make about color in your home: the small ones, the medium ones, and the large ones.\n\n<Q38>. They're the little spots of color like throw pillows, mirrors and baskets that most of us use to add visual interest to our rooms. Less tiring than painting your walls and less expensive than buying a colorful sofa, small color choices bring with them the significant benefit of being easily changeable.\n\nMedium color choices are generally furniture pieces such as sofas, dinner tables or bookshelves. <Q39>. They require a bigger commitment than smaller ones, and they have a more powerful effect on the feeling of a space.\n\nThe large color decisions in your rooms concern the walls, ceilings, and floors. Whether you're looking at wallpaper or paint, the time, effort and relative expense put into it are significant. <Q40>.", "options": ["While all of them are useful", "Whatever you're looking for", "If you're experimenting with a color", "Small color choices are the ones we're most familiar with", "It's not really a good idea to use too many small color pieces", "So it pays to be sure, because you want to get it right the first time", "Color choices in this range are a step up from the small ones in two major ways"], "question_mark": "<Q36>", "answers": {"text": "Whatever you're looking for", "option_index": 1}}, "output": "Small color choices are the ones we're most familiar with."}, "example_prompt": "If anyone had told me three years ago that I would be spending most of my weekends camping. I would have laughed heartily. Campers, in my eyes, were people who enjoyed insects bites, ill-cooked meals, and uncomfortable sleeping bags. They had nothing in common with me. <Q36> The friends who introduced me to camping thought that it meant to be a pioneer. <Q37> We sleep in a tent, cooked over an open fire, and walked a long distance to take the shower and use the bathroom. This brief visit with Mother Nature cost me two days off from work, recovering from a bad case of sunburn and the doctor's bill for my son's food poisoning. I was, nevertheless, talked into going on another fun-filled holiday in the wilderness. <Q38> Instead, we had a pop-up camper with comfortable beds and an air conditioner. My nature-loving friends had remembered to bring all the necessities of life. <Q39> We have done a lot of it since. Recently, we bought a twenty-eight-foot travel trailer complete with a bathroom and a built-in TV set. There is a separate bedroom, a modern kitchen with a refrigerator. The trailer even has matching carpet and curtains. <Q40> It must be true that sooner or later, everyone finds his or her way back to nature. I recommend that you find your way in style. \nWhat should be filled in at the <Q36> position? \"This time there was no tent.\", \"Things are going to be improved.\", \"The trip they took me on was a rough one.\", \"I was to learn a lot about camping since then, however.\", \"I must say that I have certainly come to enjoy camping.\", \"After the trip, my family became quite interested in camping.\" or \"There was no shade as the trees were no more than 3 feet tall.\"?\nI was to learn a lot about camping since then, however.\n\nIf anyone had told me three years ago that I would be spending most of my weekends camping. I would have laughed heartily. Campers, in my eyes, were people who enjoyed insects bites, ill-cooked meals, and uncomfortable sleeping bags. They had nothing in common with me. I was to learn a lot about camping since then, however. The friends who introduced me to camping thought that it meant to be a pioneer. The trip they took me on was a rough one. We sleep in a tent, cooked over an open fire, and walked a long distance to take the shower and use the bathroom. This brief visit with Mother Nature cost me two days off from work, recovering from a bad case of sunburn and the doctor's bill for my son's food poisoning. I was, nevertheless, talked into going on another fun-filled holiday in the wilderness. <Q38> Instead, we had a pop-up camper with comfortable beds and an air conditioner. My nature-loving friends had remembered to bring all the necessities of life. <Q39> We have done a lot of it since. Recently, we bought a twenty-eight-foot travel trailer complete with a bathroom and a built-in TV set. There is a separate bedroom, a modern kitchen with a refrigerator. The trailer even has matching carpet and curtains. <Q40> It must be true that sooner or later, everyone finds his or her way back to nature. I recommend that you find your way in style. \nWhat should be filled in at the <Q38> position? \"This time there was no tent.\", \"Things are going to be improved.\", \"The trip they took me on was a rough one.\", \"I was to learn a lot about camping since then, however.\", \"I must say that I have certainly come to enjoy camping.\", \"After the trip, my family became quite interested in camping.\" or \"There was no shade as the trees were no more than 3 feet tall.\"?\nThis time there was no tent.\n\nColor is fundamental in home design  something you'll always have in every room. A grasp of how to manage color in your spaces is one of the first steps to creating rooms you'll love to live in. Do you want a room that's full of life? Professional? Or are you just looking for a place to relax after a long day? <Q36>, color is the key to making a room feel the way you want it to feel. Over the years, there have been a number of different techniques to help designers approach this important point. <Q37>, they can get a little complex. But good news is that there're really only three kinds of decisions you need to make about color in your home: the small ones, the medium ones, and the large ones. <Q38>. They're the little spots of color like throw pillows, mirrors and baskets that most of us use to add visual interest to our rooms. Less tiring than painting your walls and less expensive than buying a colorful sofa, small color choices bring with them the significant benefit of being easily changeable. Medium color choices are generally furniture pieces such as sofas, dinner tables or bookshelves. <Q39>. They require a bigger commitment than smaller ones, and they have a more powerful effect on the feeling of a space. The large color decisions in your rooms concern the walls, ceilings, and floors. Whether you're looking at wallpaper or paint, the time, effort and relative expense put into it are significant. <Q40>.\nWhat should be filled in at the <Q36> position? \"While all of them are useful\", \"Whatever you're looking for\", \"If you're experimenting with a color\", \"Small color choices are the ones we're most familiar with\", \"It's not really a good idea to use too many small color pieces\", \"So it pays to be sure, because you want to get it right the first time\" or \"Color choices in this range are a step up from the small ones in two major ways\"?\n\n"}}
{"gaokao_essay_writing---default": {"description": "gaokao_essay_writing", "creator": "Promptware Authors", "homepage": "https://github.com/expressai/promptware", "reference": "", "codebase_url": "https://github.com/expressai/promptware/tree/main/softwares", "license": "apache-2.0", "task": null, "research_tasks": ["conditional-generation"], "application_categories": ["generation"], "application_subcategories": ["text-generation"], "original_platform": "gpt3", "design_pattern": "standalone", "source_language": "en", "target_language": "en", "module_name": "gaokao_essay_writing_promptware", "config_name": "default", "kernel_configs": {"openai": {"platform": "openai", "model_name": "text-davinci-002", "max_tokens": 300, "temperature": 0.0, "top_p": 1, "suffix": "", "frequency_penalty": 0.0, "presence_penalty": 0.0, "logprobs": 1, "n": 1, "echo": false, "stop": null}}, "software_configs": {"gaokao_essay_writing": {"name": "gaokao_essay_writing", "description": "Gaokao English essay writing", "instruction": "", "demonstration": ["Suppose you are Li Hua, teaching your English friend Leslie Chinese. Please write an email with your plans for the next class. The contents include: 1. Time and place; 2. Content: learning Tang poetry; 3. Preparation before class: briefly understand the history of the Tang Dynasty. Here are some requirements: 1. The number of words is about 100; 2. Details can be added appropriately to make the writing coherent.\nDear Leslie, I am very happy that you have made great progress in learning Chinese and you are interested in Chinese culture. Now I'll tell you the next learning programme. On July 20, we are going to learn poems of the Tang Dymasty which you are interested in in the Lecture Hall. As a foreign learner, it is difficult for you to understand the true meaning and the culture of them. Therefore, before class, you can read some books related to the history of the Tang Dynasty to better appreciate the poems. Be sure to go to the Lecture Hall on time. You cannot miss the wonderful poems. Best wishes. Yours, Li Hua\n", "Suppose you are Li Hua, and you want to invite Henry, a foreign teacher, to visit the Chinese paper-cutting art exhibition. Please write him an email, including: 1. Exhibition time and location; 2. Exhibition content. Here are some requirements: 1. The number of words is about 100; 2. Details can be added appropriately to make the writing coherent.\nDear Henry, I'm Li Hua, one of your students in your cultural class. I know you're interested in one of Chinese traditional art forms papercutting. So I invite you to attend an exhibition of it. It'll be held from June 10 to July 10 this year and the opening time is from 9:00 am to 7:00 pm from Monday to Saturday and the place of the exhibition is at the City Gallery, which is located at 118, Jian Guo Road, Hai Dian District. Shall we go there together this Friday afternoon? I will meet you at 2:00 pm at the teaching building gate if you like. You know we Chinese have a lot of traditional art forms, of which papercutting is one of the most popular. In the exhibition, you will enjoy many special kinds of papercuttings. Maybe you can learn one or two skills of the cutting. Looking forward to your early reply. Regards, Li Hua"], "prompt_template": "lambda input: f\"{input['source']}\",", "task": "conditional-generation"}}, "dataset_configs": null, "example": {"input": {"source": "Assuming you are Li Hua, your New Zealand friend Terry will be visiting a Chinese friend's house and emailing you about customs. Please reply to the email, including: 1. Arrival time; 2. Appropriate gifts; 3. Table manners. Here are some requirements: 1. The number of words is about 100; 2. Details can be added appropriately to make the writing coherent.", "reference": "Dear Terry, How are you doing? In your last letter, you asked me about being a guest to a Chinese friend's home. Now, I am writing to inform you of some relevant details. To begin with, according to our tradition, you are supposed to arrive early, so that you can help the family prepare the dinner, which is meaningful and interesting. Besides, you'd better bring some gifts, like a book or a Chinese knot. What's more, when you are enjoying the meal, you need to avoid making noises while chewing food. Hopefully, these suggestions would be helpful for you. I have the confidence that you will have a great time. Best wishes! Yours, Li Hua"}, "output": "Dear Terry, Thank you for your email. I am glad to hear that you will be visiting a Chinese friend's house. Here are some suggestions: 1. It is better to arrive on time or a little earlier. If you are late, it may cause inconvenience to your host. 2. It is customary to bring a small gift when visiting someone's home in China. Something like fruit or flowers is appropriate. 3. When eating at a Chinese home, it is polite to wait for the host to start eating before you begin. Also, try to finish everything on your plate, as it is considered rude to leave food uneaten. I hope you have a great time. Best wishes, Li Hua"}, "example_prompt": "Suppose you are Li Hua, teaching your English friend Leslie Chinese. Please write an email with your plans for the next class. The contents include: 1. Time and place; 2. Content: learning Tang poetry; 3. Preparation before class: briefly understand the history of the Tang Dynasty. Here are some requirements: 1. The number of words is about 100; 2. Details can be added appropriately to make the writing coherent.\nDear Leslie, I am very happy that you have made great progress in learning Chinese and you are interested in Chinese culture. Now I'll tell you the next learning programme. On July 20, we are going to learn poems of the Tang Dymasty which you are interested in in the Lecture Hall. As a foreign learner, it is difficult for you to understand the true meaning and the culture of them. Therefore, before class, you can read some books related to the history of the Tang Dynasty to better appreciate the poems. Be sure to go to the Lecture Hall on time. You cannot miss the wonderful poems. Best wishes. Yours, Li Hua\n\nSuppose you are Li Hua, and you want to invite Henry, a foreign teacher, to visit the Chinese paper-cutting art exhibition. Please write him an email, including: 1. Exhibition time and location; 2. Exhibition content. Here are some requirements: 1. The number of words is about 100; 2. Details can be added appropriately to make the writing coherent.\nDear Henry, I'm Li Hua, one of your students in your cultural class. I know you're interested in one of Chinese traditional art forms papercutting. So I invite you to attend an exhibition of it. It'll be held from June 10 to July 10 this year and the opening time is from 9:00 am to 7:00 pm from Monday to Saturday and the place of the exhibition is at the City Gallery, which is located at 118, Jian Guo Road, Hai Dian District. Shall we go there together this Friday afternoon? I will meet you at 2:00 pm at the teaching building gate if you like. You know we Chinese have a lot of traditional art forms, of which papercutting is one of the most popular. In the exhibition, you will enjoy many special kinds of papercuttings. Maybe you can learn one or two skills of the cutting. Looking forward to your early reply. Regards, Li Hua\nAssuming you are Li Hua, your New Zealand friend Terry will be visiting a Chinese friend's house and emailing you about customs. Please reply to the email, including: 1. Arrival time; 2. Appropriate gifts; 3. Table manners. Here are some requirements: 1. The number of words is about 100; 2. Details can be added appropriately to make the writing coherent.\n"}}
{"gaokao_grammar_correction---default": {"description": "Gaokao English Grammar Correction", "creator": "Promptware Authors", "homepage": "https://github.com/expressai/promptware", "reference": "", "codebase_url": "https://github.com/expressai/promptware/tree/main/softwares", "license": "apache-2.0", "task": null, "research_tasks": ["grammatical-error-correction"], "application_categories": ["transformation"], "application_subcategories": ["rewriting"], "original_platform": "gpt3", "design_pattern": "standalone", "source_language": "en", "target_language": "en", "module_name": "gaokao_grammar_correction_promptware", "config_name": "default", "kernel_configs": {"openai": {"platform": "openai", "model_name": "text-davinci-002", "max_tokens": 200, "temperature": 0.0, "top_p": 1, "suffix": "", "frequency_penalty": 0.0, "presence_penalty": 0.0, "logprobs": 1, "n": 1, "echo": false, "stop": null}}, "software_configs": {"gaokao_grammar_correction": {"name": "gaokao_grammar_correction", "description": "Gaokao English Grammar Correction", "instruction": "", "demonstration": ["In the summer holiday following my eighteen birthday, I took driving lessons. I still remember how hard first day was. Before getting into the car, I thought I had learned the instructor's orders, so once I started the car, my mind goes blank, I forgot what he had said to me altogether. The instructor kept repeating the word, \"Speed up!\" \"Slow down!\" \"Turning left!\" I was so much nervous that I could hardly tell which direction was left. A few minutes late, the instructor asked me to stop the car. It was a relief and I came to a suddenly stop just in the middle on the road.\nPlease fix the grammatical errors in the above paragraph.\nIn the summer holiday following my eighteenth birthday, I took driving lessons. I still remember how hard the first day was. Before getting into the car, I thought I had learned the instructor's orders, but once I started the car, my mind went blank, I forgot what he had said to me altogether. The instructor kept repeating the words, \"Speed up!\" \"Slow down!\" \"Turn left!\" I was so nervous that I could hardly tell which direction was left. A few minutes later, the instructor asked me to stop the car. It was a relief and I came to a sudden stop just in the middle on the road.\n", "Mr. and Mrs. Zhang all work in our school. They live far from the school, and it takes them about a hour and a half to go to work every day. In their spare time, they are interesting in planting vegetables in their garden, that is on the rooftop of their house. They often get up earlier and water the vegetables together. They have also bought for some gardening tools. Beside, they often get some useful informations from the Internet. When summer came, they will invite their students pick the fresh vegetables!\nPlease fix the grammatical errors in the above paragraph.\nMr. and Mrs. Zhang both work in our school. They live far from the school, and it takes them about an hour and a half to go to work every day. In their spare time, they are interested in planting vegetables in their garden, which is on the rooftop of their house. They often get up early and water the vegetables together. They have also bought some gardening tools. Besides, they often get some useful information from the Internet. When summer comes, they will invite their students to pick the fresh vegetables!\n"], "prompt_template": "lambda input: f\"{input['text']}\\nPlease fix the\"", "task": "grammatical-error-correction"}}, "dataset_configs": null, "example": {"input": {"text": "During my last winter holiday, I went to countryside with my father to visit my grandparents. I find a big change there. The first time I went there, they were living in a small house with dogs, ducks, and another animals. Last winter when I went here again, they had a big separate house to raise dozens of chicken. They also had a small pond which they raised fish. My grandpa said last summer they earned quite a lot by sell the fish. I felt happily that their life had improved. At the end of our trip, I told my father that I planned to return for every two years, but he agreed.", "edits": {"start_idx": [8, 17, 39, 46, 58, 65, 65, 80, 85, 110], "end_idx": [8, 18, 40, 47, 59, 65, 66, 81, 86, 111], "corrections": [["the"], ["found"], ["other"], ["there"], ["chickens."], ["in"], ["which", "where"], ["selling"], ["happy"], ["and"]]}}, "output": "During my last winter holiday, I went to the countryside with my father to visit my grandparents. I found a big change there. The first time I went there, they were living in a small house with dogs, ducks, and other animals. Last winter when I went there again, they had a big separate house to raise dozens of chickens. They also had a small pond which they raised fish in. My grandpa said that last summer they earned quite a lot by selling the fish. I felt happily that their life had improved. At the end of our trip, I told my father that I planned to return every two years, but he agreed."}, "example_prompt": "In the summer holiday following my eighteen birthday, I took driving lessons. I still remember how hard first day was. Before getting into the car, I thought I had learned the instructor's orders, so once I started the car, my mind goes blank, I forgot what he had said to me altogether. The instructor kept repeating the word, \"Speed up!\" \"Slow down!\" \"Turning left!\" I was so much nervous that I could hardly tell which direction was left. A few minutes late, the instructor asked me to stop the car. It was a relief and I came to a suddenly stop just in the middle on the road.\nPlease fix the grammatical errors in the above paragraph.\nIn the summer holiday following my eighteenth birthday, I took driving lessons. I still remember how hard the first day was. Before getting into the car, I thought I had learned the instructor's orders, but once I started the car, my mind went blank, I forgot what he had said to me altogether. The instructor kept repeating the words, \"Speed up!\" \"Slow down!\" \"Turn left!\" I was so nervous that I could hardly tell which direction was left. A few minutes later, the instructor asked me to stop the car. It was a relief and I came to a sudden stop just in the middle on the road.\n\nMr. and Mrs. Zhang all work in our school. They live far from the school, and it takes them about a hour and a half to go to work every day. In their spare time, they are interesting in planting vegetables in their garden, that is on the rooftop of their house. They often get up earlier and water the vegetables together. They have also bought for some gardening tools. Beside, they often get some useful informations from the Internet. When summer came, they will invite their students pick the fresh vegetables!\nPlease fix the grammatical errors in the above paragraph.\nMr. and Mrs. Zhang both work in our school. They live far from the school, and it takes them about an hour and a half to go to work every day. In their spare time, they are interested in planting vegetables in their garden, which is on the rooftop of their house. They often get up early and water the vegetables together. They have also bought some gardening tools. Besides, they often get some useful information from the Internet. When summer comes, they will invite their students to pick the fresh vegetables!\n\nDuring my last winter holiday, I went to countryside with my father to visit my grandparents. I find a big change there. The first time I went there, they were living in a small house with dogs, ducks, and another animals. Last winter when I went here again, they had a big separate house to raise dozens of chicken. They also had a small pond which they raised fish. My grandpa said last summer they earned quite a lot by sell the fish. I felt happily that their life had improved. At the end of our trip, I told my father that I planned to return for every two years, but he agreed.\nPlease fix the grammatical errors in the above paragraph.\n\n"}}
{"gaokao_listening---default": {"description": "gaokao_listening", "creator": "Promptware Authors", "homepage": "https://github.com/expressai/promptware", "reference": "", "codebase_url": "https://github.com/expressai/promptware/tree/main/softwares", "license": "apache-2.0", "task": null, "research_tasks": ["qa-multiple-choice"], "application_categories": ["conversation"], "application_subcategories": ["question-answering"], "original_platform": "gpt3", "design_pattern": "standalone", "source_language": "en", "target_language": "en", "module_name": "gaokao_listening_promptware", "config_name": "default", "kernel_configs": {"openai": {"platform": "openai", "model_name": "text-davinci-002", "max_tokens": 20, "temperature": 0.0, "top_p": 1, "suffix": "", "frequency_penalty": 0.0, "presence_penalty": 0.0, "logprobs": 1, "n": 1, "echo": false, "stop": null}}, "software_configs": {"gaokao_listening": {"name": "gaokao_listening", "description": "Gaokao English Listening", "instruction": "", "demonstration": ["The woman said: \"Have you seen the movie \"Hangover\"? We went to see it last night.\" The man said: \"How was it?\" The woman said: \"Jason thought it was extremely amusing, but I was a bit disappointed.\"\nWhat does the woman think of the movie? \"It's amusing.\", \"It's exciting.\" or \"It's disappointing.\"?\nIt's disappointing.\n", "The man said: \"Susan, I heard you are going to France. How long will you be staying there?\" The woman said: \"A whole year. My aunt lives there. I'm going to do a one-month course at a language school and spend the rest of the time traveling.\"\nHow will Susan spend most of her time in France? \"Traveling around.\", \"Studying at a school.\" or \"Looking after her aunt.\"?\nTraveling around.\n"], "prompt_template": "lambda input: f\"{preprocess_context(input['context'])}\\n\"", "task": "qa-multiple-choice"}}, "dataset_configs": null, "example": {"input": {"context": "Person 2: \"James. You've been watching TV for the whole evening what's on?\" Person 1: \"It's a science programme on the origin of the universe. I'll give a presentation on it in my class tomorrow.\"", "context_oracle": "W: \"James, you've been watching TV for the whole evening. What's on?\" M: \"It's a science program on the origin of the universe. I'll give a presentation on it in my class tomorrow.\"", "options": ["Watch a TV program.", "Give a talk.", "Write a report."], "question": "what will James do tomorrow?", "answers": {"text": "Give a talk.", "option_index": 1}}, "output": "Give a talk."}, "example_prompt": "The woman said: \"Have you seen the movie \"Hangover\"? We went to see it last night.\" The man said: \"How was it?\" The woman said: \"Jason thought it was extremely amusing, but I was a bit disappointed.\"\nWhat does the woman think of the movie? \"It's amusing.\", \"It's exciting.\" or \"It's disappointing.\"?\nIt's disappointing.\n\nThe man said: \"Susan, I heard you are going to France. How long will you be staying there?\" The woman said: \"A whole year. My aunt lives there. I'm going to do a one-month course at a language school and spend the rest of the time traveling.\"\nHow will Susan spend most of her time in France? \"Traveling around.\", \"Studying at a school.\" or \"Looking after her aunt.\"?\nTraveling around.\n\nPerson 2: \"James. You've been watching TV for the whole evening what's on?\" Person 1: \"It's a science programme on the origin of the universe. I'll give a presentation on it in my class tomorrow.\"\nwhat will James do tomorrow? \"Watch a TV program.\", \"Give a talk.\" or \"Write a report.\"?\n"}}
{"gaokao_reading_comprehension---default": {"description": "gaokao_reading_comprehension", "creator": "Promptware Authors", "homepage": "https://github.com/expressai/promptware", "reference": "", "codebase_url": "https://github.com/expressai/promptware/tree/main/softwares", "license": "apache-2.0", "task": null, "research_tasks": ["qa-multiple-choice"], "application_categories": ["conversation"], "application_subcategories": ["question-answering"], "original_platform": "gpt3", "design_pattern": "standalone", "source_language": "en", "target_language": "en", "module_name": "gaokao_reading_comprehension_promptware", "config_name": "default", "kernel_configs": {"openai": {"platform": "openai", "model_name": "text-davinci-002", "max_tokens": 20, "temperature": 0.0, "top_p": 1, "suffix": "", "frequency_penalty": 0.0, "presence_penalty": 0.0, "logprobs": 1, "n": 1, "echo": false, "stop": null}}, "software_configs": {"gaokao_reading_comprehension": {"name": "gaokao_reading_comprehension", "description": "Gaokao English Reading Comprehension", "instruction": "", "demonstration": ["I work with Volunteers for Wildlife, a rescue and education organization at Bailey Arboretum in Locust Valley. Trying to help injured, displaced or sick creatures can be heartbreaking; survival is never certain. However, when it works, it is simply beautiful. I got a rescue call from a woman in Muttontown. She had found a young owl on the ground. When I arrived, I saw a 2-to 3-week-old owl. It had already been placed in a carrier for safety. I examined the chick and it seemed fine. If I could locate the nest, I might have been able to put it back, but no luck. My next work was to construct a nest and anchor it in a tree. The homeowner was very helpful. A wire basket was found. I put some pine branches into the basket to make this nest safe and comfortable. I placed the chick in the nest, and it quickly calmed down. Now all that was needed were the parents, but they were absent. I gave the homeowner a recording of the hunger screams of owl chicks. These advertise the presence of chicks to adults; they might also encourage our chick to start calling as well. I gave the owner as much information as possible and headed home to see what news the night might bring. A nervous night to be sure, but sometimes the spirits of nature smile on us all! The homeowner called to say that the parents had responded to the recordings. I drove over and saw the chick in the nest looking healthy and active. And it was accompanied in the nest by the greatest sight of all \u2014 LUNCH! The parents had done their duty and would probably continue to do so.\nWhat is unavoidable in the author's rescue work according to paragraph 1? \"Efforts made in vain.\", \"Getting injured in his work.\", \"Feeling uncertain about his future.\" or \"Creatures forced out of their homes.\"?\nEfforts made in vain.\n", "Some of the world's most famous musicians recently gathered in Paris and New Orleans to celebrate the first annual International Jazz Day. UNESCO (United Nations Educational, Scientific and Cultural Organization) recently set April 30 as a day to raise awareness of jazz music, its significance, and its potential as a unifying voice across cultures. Despite the celebrations, though, in the U.S. the jazz audience continues to shrink and grow older, and the music has failed to connect with younger generations. It's Jason Moran's job to help change that. As the Kennedy Center's artistic adviser for jazz, Moran hopes to widen the audience for jazz, make the music more accessible, and preserve its history and culture. \"Jazz seems like it's not really a part of the American appetite,\" Moran tells National Public Radio's reporter Neal Conan. \"What I'm hoping to accomplish is that mu generation and younger start to reconsider and understand that jazz is not black and write anymore. It's actually color, and it's actually digital.\" Moran says one of the problems with jazz today is that the entertainment aspect of the music has been lost. \"The music can't be presented today the way it was in 1908 or 1958. It has to continue to move, because the way the world works is not the same,\" says Moran. Last year, Moran worked on a project that arranged Fats Waller's music for a dance party, \"Just to kind of put it back in the mind that Waller is dance music as much as it is concert music,\" says Moran. \"For me, it's the recontextualization. In music, where does the emotion lie? Are we, as abstract as a Charlie Parker record gets us into a dialogue about our emotions and our thoughts? Sometimes we lose sight that the music has a wider context,\" says Moran, \"So I want to continue those dialogue. Those are the things I want to foster.\"\nWhy did UNESCO set April 30 as International Jazz Day? \"To remember the birth of jazz.\", \"To protect cultural diversity.\", \"To encourage people to study music.\" or \"To recognize the value of jazz.\"?\nTo recognize the value of jazz.\n"], "prompt_template": "lambda input: f\"{preprocess_context(input['context'])}\\n\"", "task": "qa-multiple-choice"}}, "dataset_configs": null, "example": {"input": {"context": "Washington, D.C. Bicycle Tours\n\nCherry Blossom Bike Tour in Washington, D.C.\n\nDuration: 3 hours\n\nThis small group bike tour is a fantastic way to see the world-famous cherry trees with beautiful flowers of Washington, D.C. Your guide will provide a history lesson about the trees and the famous monuments where they blossom. Reserve your spot before availability \u2014 and the cherry blossoms \u2014 disappear!\n\nWashington Capital Monuments Bicycle Tour\n\nDuration: 3 hours (4 miles)\n\nJoin a guided bike tour and view some of the most popular monuments in Washington, D. C. Explore the monuments and memorials on the National Mall as your guide shares unique facts and factory at each stop.\n\nCapital City Bike Tour in Washington, D. C.\n\nDuration: 3 hours\n\nMorning or Afternoon, this bike tour is the perfect tour for D.C. newcomers and locals looking to experience Washington, D. C. in a healthy way with minimum effort. Knowledgeable guides will entertain you with the most interesting stories about Presidents, Congress, memorials, and parks. Comfortable bikes and a smooth tour route make cycling between the sites fun and relaxing.\n\nWashington Capital Sites at Night Bicycle Tour\n\nDuration: 3 hours (7 miles)\n\nJoin a small group bike tour for an evening of exploration in the heart of Washington, D. C. Get up close to the monuments and memorials as you bike the sites of Capitol Hill and the National Mall. Frequent stops are made for photo taking as your guide offers unique facts and history. Tour includes bike, helmet, and bottled water. All rides are equipped with reflective vests and safety lights.", "options": ["Cherry Blossom Bike Tour in Washington, D. C.", "Washington Capital Monuments Bicycle Tour.", "Capital City Bike Tour in Washington, D. C.", "Washington Capital Sites at Night Bicycle Tour."], "question": "Which tour do you need to book in advance?", "answers": {"text": "Cherry Blossom Bike Tour in Washington, D. C.", "option_index": 0}}, "output": "Cherry Blossom Bike Tour in Washington, D. C."}, "example_prompt": "I work with Volunteers for Wildlife, a rescue and education organization at Bailey Arboretum in Locust Valley. Trying to help injured, displaced or sick creatures can be heartbreaking; survival is never certain. However, when it works, it is simply beautiful. I got a rescue call from a woman in Muttontown. She had found a young owl on the ground. When I arrived, I saw a 2-to 3-week-old owl. It had already been placed in a carrier for safety. I examined the chick and it seemed fine. If I could locate the nest, I might have been able to put it back, but no luck. My next work was to construct a nest and anchor it in a tree. The homeowner was very helpful. A wire basket was found. I put some pine branches into the basket to make this nest safe and comfortable. I placed the chick in the nest, and it quickly calmed down. Now all that was needed were the parents, but they were absent. I gave the homeowner a recording of the hunger screams of owl chicks. These advertise the presence of chicks to adults; they might also encourage our chick to start calling as well. I gave the owner as much information as possible and headed home to see what news the night might bring. A nervous night to be sure, but sometimes the spirits of nature smile on us all! The homeowner called to say that the parents had responded to the recordings. I drove over and saw the chick in the nest looking healthy and active. And it was accompanied in the nest by the greatest sight of all \u2014 LUNCH! The parents had done their duty and would probably continue to do so.\nWhat is unavoidable in the author's rescue work according to paragraph 1? \"Efforts made in vain.\", \"Getting injured in his work.\", \"Feeling uncertain about his future.\" or \"Creatures forced out of their homes.\"?\nEfforts made in vain.\n\nSome of the world's most famous musicians recently gathered in Paris and New Orleans to celebrate the first annual International Jazz Day. UNESCO (United Nations Educational, Scientific and Cultural Organization) recently set April 30 as a day to raise awareness of jazz music, its significance, and its potential as a unifying voice across cultures. Despite the celebrations, though, in the U.S. the jazz audience continues to shrink and grow older, and the music has failed to connect with younger generations. It's Jason Moran's job to help change that. As the Kennedy Center's artistic adviser for jazz, Moran hopes to widen the audience for jazz, make the music more accessible, and preserve its history and culture. \"Jazz seems like it's not really a part of the American appetite,\" Moran tells National Public Radio's reporter Neal Conan. \"What I'm hoping to accomplish is that mu generation and younger start to reconsider and understand that jazz is not black and write anymore. It's actually color, and it's actually digital.\" Moran says one of the problems with jazz today is that the entertainment aspect of the music has been lost. \"The music can't be presented today the way it was in 1908 or 1958. It has to continue to move, because the way the world works is not the same,\" says Moran. Last year, Moran worked on a project that arranged Fats Waller's music for a dance party, \"Just to kind of put it back in the mind that Waller is dance music as much as it is concert music,\" says Moran. \"For me, it's the recontextualization. In music, where does the emotion lie? Are we, as abstract as a Charlie Parker record gets us into a dialogue about our emotions and our thoughts? Sometimes we lose sight that the music has a wider context,\" says Moran, \"So I want to continue those dialogue. Those are the things I want to foster.\"\nWhy did UNESCO set April 30 as International Jazz Day? \"To remember the birth of jazz.\", \"To protect cultural diversity.\", \"To encourage people to study music.\" or \"To recognize the value of jazz.\"?\nTo recognize the value of jazz.\n\nWashington, D.C. Bicycle Tours Cherry Blossom Bike Tour in Washington, D.C. Duration: 3 hours This small group bike tour is a fantastic way to see the world-famous cherry trees with beautiful flowers of Washington, D.C. Your guide will provide a history lesson about the trees and the famous monuments where they blossom. Reserve your spot before availability  and the cherry blossoms  disappear! Washington Capital Monuments Bicycle Tour Duration: 3 hours (4 miles) Join a guided bike tour and view some of the most popular monuments in Washington, D. C. Explore the monuments and memorials on the National Mall as your guide shares unique facts and factory at each stop. Capital City Bike Tour in Washington, D. C. Duration: 3 hours Morning or Afternoon, this bike tour is the perfect tour for D.C. newcomers and locals looking to experience Washington, D. C. in a healthy way with minimum effort. Knowledgeable guides will entertain you with the most interesting stories about Presidents, Congress, memorials, and parks. Comfortable bikes and a smooth tour route make cycling between the sites fun and relaxing. Washington Capital Sites at Night Bicycle Tour Duration: 3 hours (7 miles) Join a small group bike tour for an evening of exploration in the heart of Washington, D. C. Get up close to the monuments and memorials as you bike the sites of Capitol Hill and the National Mall. Frequent stops are made for photo taking as your guide offers unique facts and history. Tour includes bike, helmet, and bottled water. All rides are equipped with reflective vests and safety lights.\nWhich tour do you need to book in advance? \"Cherry Blossom Bike Tour in Washington, D. C.\", \"Washington Capital Monuments Bicycle Tour.\", \"Capital City Bike Tour in Washington, D. C.\" or \"Washington Capital Sites at Night Bicycle Tour.\"?\n"}}
{"grammar_correction---default": {"description": "Corrects sentences into standard English.", "creator": "OpenAI", "homepage": "https://beta.openai.com/examples/default-grammar", "reference": "", "codebase_url": "https://beta.openai.com/examples/default-grammar", "license": "no-license", "task": null, "research_tasks": ["conditional-generation"], "application_categories": ["transformation"], "application_subcategories": ["rewriting"], "original_platform": "gpt3", "design_pattern": "standalone", "source_language": "en", "target_language": "en", "module_name": "grammar_correction_promptware", "config_name": "default", "kernel_configs": {"openai": {"platform": "openai", "model_name": "text-curie-001", "max_tokens": 60, "temperature": 0, "top_p": 1, "suffix": "", "frequency_penalty": 0.0, "presence_penalty": 0.0, "logprobs": 1, "n": 1, "echo": false, "stop": null}}, "software_configs": {"grammar_correction": {"name": "grammar_correction", "description": "Corrects sentences into standard English.", "instruction": "Correct this to standard English:", "demonstration": [], "prompt_template": "lambda input: f\"{input['text']}\\n\",", "task": "conditional-generation"}}, "dataset_configs": null, "example": {"input": {"text": "She no went to the market."}, "output": "She didn't go to the market."}, "example_prompt": "Correct this to standard English:\n\nShe no went to the market.\n\n"}}
{"headline_generation---default": {"description": "This software is used to generate headline for a news article with different styles", "creator": "Promptware Authors", "homepage": "https://github.com/expressai/promptware", "reference": "", "codebase_url": "https://github.com/expressai/promptware/tree/main/softwares", "license": "apache-2.0", "task": null, "research_tasks": ["conditional-generation"], "application_categories": ["transformation"], "application_subcategories": ["summarization"], "original_platform": "gpt3", "design_pattern": "standalone", "source_language": "en", "target_language": "en", "module_name": "headline_generation_promptware", "config_name": "default", "kernel_configs": {"openai": {"platform": "openai", "model_name": "text-davinci-002", "max_tokens": 64, "temperature": 0, "top_p": 1, "suffix": "", "frequency_penalty": 0.0, "presence_penalty": 0.0, "logprobs": 1, "n": 1, "echo": false, "stop": null}}, "software_configs": {"headline_generation": {"name": "headline_generation", "description": "This software is used to generate headline for a news article with different styles", "instruction": "\u7ed9\u5b9a\u4e00\u4e2a\u65b0\u95fb\u6587\u7ae0\uff0c\u6211\u4eec\u751f\u6210\u4e00\u4e2a\u8db3\u591f\u5438\u5f15\u4eba\u7684\u6807\u9898", "demonstration": ["\u65b0\u95fb\uff1a\n\u8fd1\u65e5\uff0c\u73b0\u5e7447\u5c81\u7684\u65b0\u664b\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb\u3001\u6b66\u6c49\u5927\u5b66\u6559\u6388\u5b8b\u4fdd\u4eae\u56e2\u961f\u4e8eNature\uff08\u300a\u81ea\u7136\u300b\uff09\u6742\u5fd7**\u53d1\u8868\u8bba\u6587\uff0c\u53d1\u73b0\u4fc3\u4f7f\u80c6\u56fa\u9187\u5916\u6392\u800c\u964d\u8102\u7684\u5168\u65b0\u7b56\u7565\u30022021\u5e7411\u6708\uff0c\u5b8b\u4fdd\u4eae\u5f53\u9009\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb\uff0c\u65f6\u5e7446\u5c81\uff0c\u6210\u4e3a\u5f53\u5c4a\u5f53\u9009\u7684\u6700\u5e74\u8f7b\u9662\u58eb\u4e4b\u4e00\u3002\u5b8b\u4fdd\u4eae\u6559\u6388\u66fe\u5728\u6b66\u5927\u7814\u7a76\u751f\u5f00\u5b66\u5178\u793c\u4e0a\u6f14\u8bb2\uff0c\u7528\u81ea\u5df1\u7684\u6210\u957f\u53d1\u5c55\u7ecf\u5386\u6fc0\u52b1\u5e7f\u5927\u7814\u7a76\u751f\uff0c\u5e76\u63d0\u51fa\u5feb\u901f\u878d\u5165\u7814\u7a76\u751f\u751f\u6d3b\u7684\u516d\u4e2a\u5efa\u8bae\u3002\n\u6807\u9898\uff1a\n47\u5c81\u9662\u58eb\u518d\u53d1Nature\uff01\u4ed6\u7ed9\u7814\u7a76\u751f\u7684\u8fd96\u4e2a\u5efa\u8bae\u6d41\u4f20\u751a\u5e7f", "\u65b0\u95fb\uff1a\n\u65e5\u524d\uff0c\u6559\u80b2\u90e8\u5b98\u7f51\u53d1\u5e03\u300a\u5173\u4e8e\u6cb3\u5317\u5de5\u7a0b\u6280\u672f\u5b66\u9662\u7b49\u56db\u6240\u6c11\u529e\u672c\u79d1\u5b66\u6821\u53d8\u66f4\u4e3e\u529e\u8005\u3001\u529e\u5b66\u5730\u5740\u7684\u516c\u793a\u300b\u3002\u300a\u516c\u793a\u300b\u900f\u9732\uff1a\u897f\u5357\u4ea4\u901a\u5927\u5b66\u5e0c\u671b\u5b66\u9662\u529e\u5b66\u5730\u5740\u7531\u201c\u56db\u5ddd\u7701\u5357\u5145\u5e02\u5609\u9675\u533a\u4e8e\u965b\u8def\u201d\u53d8\u66f4\u4e3a\u201c\u56db\u5ddd\u7701\u6210\u90fd\u5e02\u91d1\u5802\u53bf\u5b66\u5e9c\u5927\u9053558\u53f7\u201d\u3002\n\u6807\u9898\uff1a\u53c8\u4e00\u9ad8\u6821\uff0c\u8fc1\u81f3\u7701\u4f1a\u529e\u5b66\uff01", "\u65b0\u95fb\uff1a\nDichtel\u3001\u52a0\u5229\u798f\u5c3c\u4e9a\u5927\u5b66\u6d1b\u6749\u77f6\u5206\u6821K. N. Houk\u9662\u58eb\u53ca\u4e2d\u56fd\u79d1\u5b66\u9662\u4e0a\u6d77\u6709\u673a\u5316\u5b66\u7814\u7a76\u6240\u859b\u5c0f\u677e\u7b49\u591a\u56e2\u961f\u5408\u4f5c\uff08\u897f\u5317\u5927\u5b66Brittany Trang\u548c\u5929\u6d25\u5927\u5b66Li Yuli\u662f\u5171\u540c\u7b2c\u4e00\u4f5c\u8005\uff09\u5728Science\u5728\u7ebf\u53d1\u8868\u9898\u4e3a\u201cLow-temperature mineralization of perfluorocarboxylic acids\u201d\u7684\u7814\u7a76\u8bba\u6587\uff0c\u8be5\u7814\u7a76\u53d1\u73b0\u5168\u6c1f\u70f7\u57fa\u7fa7\u9178 (PFCA) \u53ef\u4ee5\u901a\u8fc7\u6c22\u6c27\u5316\u94a0\u4ecb\u5bfc\u7684\u8131\u6c1f\u9014\u5f84\u8fdb\u884c\u77ff\u5316\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u5929\u6d25\u5927\u5b66\u7406\u5b66\u96622018\u7ea7\u672c\u79d1\u751f\u674e\u9884\u7acb\u4f5c\u4e3a\u5171\u540c\u7b2c\u4e00\u4f5c\u8005\uff0c\u800c\u4ed6\u662f\u4e00\u540d\u201c00\u540e\u201d\u3002\n\u6807\u9898\uff1a\n\u786c\u6838\uff01\u201c00\u540e\u201d\u672c\u79d1\u751f\uff0c\u4e00\u4f5c\u53d1Science\uff01", "\u65b0\u95fb\uff1a\n8\u670816\u65e5\uff0c\u6b63\u503c\u592a\u539f\u7406\u5de5\u5927\u5b66\u559c\u8fce\u7eaa\u5ff5\u5efa\u6821120\u5468\u5e74\u4e4b\u9645\uff0c\u4e2d\u56fd\u79d1\u5b66\u966219\u540d\u9662\u58eb\u53ca\u4e13\u5bb6\u4e00\u884c\u5230\u592a\u539f\u7406\u5de5\u5927\u5b66\u660e\u5411\u6821\u533a\u8003\u5bdf\u8c03\u7814\uff0c\u6821\u515a\u59d4\u4e66\u8bb0\u90d1\u5f3a\u7b49\u6821\u9886\u5bfc\u966a\u540c\u8c03\u7814\u5ea7\u8c08\u3002\n\u6807\u9898\uff1a\n\u5927\u52a8\u4f5c\uff0119\u4f4d\u9662\u58eb\uff0c\u8c03\u7814\u592a\u539f\u7406\u5de5"], "prompt_template": "lambda input: f\"\u65b0\u95fb\uff1a\\n{input['text']}\\n\u6807\u9898\uff1a\\n\",", "task": "conditional-generation"}}, "dataset_configs": null, "example": {"input": {"text": "\u4f20\u7edf\u751f\u6d3b\u65b9\u5f0f\u76f8\u5173\u7684\u98ce\u9669\u56e0\u7d20\uff0c\u5305\u62ec\u5438\u70df\u3001\u8fc7\u5ea6\u996e\u9152\u3001\u7f3a\u4e4f\u8fd0\u52a8\u3001\u4e0d\u826f\u996e\u98df\u4e60\u60ef\u548c\u80a5\u80d6\uff0c\u4e0e\u6b7b\u4ea1\u98ce\u9669\u589e\u52a0\u6709\u5173\uff0c\u5c24\u5176\u662f\u6162\u6027\u75c5\u3002\u7136\u800c\uff0c\u53ea\u6709\u5c11\u6570\u7814\u7a76\u8bc4\u4f30\u4e86\u4e2a\u4eba\u751f\u6d3b\u65b9\u5f0f\u56e0\u7d20\uff08\u5982\u5438\u70df\u548c\u996e\u9152\uff09\u5bf9\u4e2d\u56fd\u4eba\u53e3\u9884\u671f\u5bff\u547d\u7684\u5f71\u54cd\u3002\u7efc\u5408\u751f\u6d3b\u65b9\u5f0f\u884c\u4e3a\u5bf9\u4e2d\u56fd\u4eba\u9884\u671f\u5bff\u547d\u7684\u5f71\u54cd\u5c1a\u4e0d\u6e05\u695a\uff0c\u9700\u8981\u586b\u8865\u8bc1\u636e\u7a7a\u767d\u30022022\u5e748\u67081\u65e5\uff0c\u5317\u4eac\u5927\u5b66\u674e\u7acb\u660e\u3001\u5415\u7b60\u4e0e\u4e2d\u56fd\u75be\u75c5\u9884\u9632\u63a7\u5236\u4e2d\u5fc3\u8d75\u4e3d\u4e91\u56e2\u961f\u5728\u00a0Lancet Public Health\u00a0(IF=72) \u671f\u520a\u5728\u7ebf\u53d1\u8868\u9898\u4e3a\u201cHealthy lifestyle and life expectancy at age 30 years in the Chinese population: an observational study\u201d\u7684\u7814\u7a76\u8bba\u6587\uff0c\u8be5\u7814\u7a76\u8c03\u67e5\u4e86\u5065\u5eb7\u751f\u6d3b\u65b9\u5f0f\u5bf9\u4e2d\u56fd\u4eba30\u5c81\u65f6\u9884\u671f\u5bff\u547d\u5f71\u54cd\u3002\u4e94\u79cd\u4f4e\u98ce\u9669\u751f\u6d3b\u65b9\u5f0f\uff1a\u4ece\u4e0d\u5438\u70df\u6216\u6212\u70df\u3001\u4e0d\u8fc7\u5ea6\u996e\u9152\u3001\u79ef\u6781\u8fd0\u52a8\u3001\u5065\u5eb7\u7684\u996e\u98df\u4e60\u60ef\u548c\u5065\u5eb7\u7684\u4f53\u578b\uff09\u4e2d\uff0c\u4e0e\u4e94\u79cd\u751f\u6d3b\u65b9\u5f0f\u5747\u4f4e\u98ce\u9669\u548c\u53ea\u6709\u4e00\u4e2a\u6216\u6ca1\u6709\u4f4e\u98ce\u9669\u7684\u4e2a\u4f53\u76f8\u6bd4\uff0c\u7537\u6027\u572830\u5c81\u65f6\u7684\u9884\u671f\u5bff\u547d\u5ef6\u957f8.8\u5e74\uff0895% CI 6.8-10.7\uff09\uff0c\u5973\u6027\u4e3a8.1\u5e74\uff086.5\u20139.9\uff09\u3002\u8be5\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7\u516c\u5171\u536b\u751f\u5e72\u9884\u8fd9\u4e94\u79cd\u5065\u5eb7\u751f\u6d3b\u65b9\u5f0f\u53ef\u80fd\u4f1a\u5927\u5e45\u5ef6\u957f\u4e2d\u56fd\u4eba\u53e3\u7684\u9884\u671f\u5bff\u547d\u3002"}, "output": "\u4e2d\u56fd\u4eba\uff0c\u4e0d\u53ea\u8981\u201c\u5403\u201d\uff01\u8fd95\u79cd\u751f\u6d3b\u65b9\u5f0f\uff0c\u53ef\u5ef6\u5bff8-9\u5e74"}, "example_prompt": "\u7ed9\u5b9a\u4e00\u4e2a\u65b0\u95fb\u6587\u7ae0\uff0c\u6211\u4eec\u751f\u6210\u4e00\u4e2a\u8db3\u591f\u5438\u5f15\u4eba\u7684\u6807\u9898\n\u65b0\u95fb\uff1a\n\u8fd1\u65e5\uff0c\u73b0\u5e7447\u5c81\u7684\u65b0\u664b\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb\u3001\u6b66\u6c49\u5927\u5b66\u6559\u6388\u5b8b\u4fdd\u4eae\u56e2\u961f\u4e8eNature\uff08\u300a\u81ea\u7136\u300b\uff09\u6742\u5fd7**\u53d1\u8868\u8bba\u6587\uff0c\u53d1\u73b0\u4fc3\u4f7f\u80c6\u56fa\u9187\u5916\u6392\u800c\u964d\u8102\u7684\u5168\u65b0\u7b56\u7565\u30022021\u5e7411\u6708\uff0c\u5b8b\u4fdd\u4eae\u5f53\u9009\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb\uff0c\u65f6\u5e7446\u5c81\uff0c\u6210\u4e3a\u5f53\u5c4a\u5f53\u9009\u7684\u6700\u5e74\u8f7b\u9662\u58eb\u4e4b\u4e00\u3002\u5b8b\u4fdd\u4eae\u6559\u6388\u66fe\u5728\u6b66\u5927\u7814\u7a76\u751f\u5f00\u5b66\u5178\u793c\u4e0a\u6f14\u8bb2\uff0c\u7528\u81ea\u5df1\u7684\u6210\u957f\u53d1\u5c55\u7ecf\u5386\u6fc0\u52b1\u5e7f\u5927\u7814\u7a76\u751f\uff0c\u5e76\u63d0\u51fa\u5feb\u901f\u878d\u5165\u7814\u7a76\u751f\u751f\u6d3b\u7684\u516d\u4e2a\u5efa\u8bae\u3002\n\u6807\u9898\uff1a\n47\u5c81\u9662\u58eb\u518d\u53d1Nature\uff01\u4ed6\u7ed9\u7814\u7a76\u751f\u7684\u8fd96\u4e2a\u5efa\u8bae\u6d41\u4f20\u751a\u5e7f\n\u65b0\u95fb\uff1a\n\u65e5\u524d\uff0c\u6559\u80b2\u90e8\u5b98\u7f51\u53d1\u5e03\u300a\u5173\u4e8e\u6cb3\u5317\u5de5\u7a0b\u6280\u672f\u5b66\u9662\u7b49\u56db\u6240\u6c11\u529e\u672c\u79d1\u5b66\u6821\u53d8\u66f4\u4e3e\u529e\u8005\u3001\u529e\u5b66\u5730\u5740\u7684\u516c\u793a\u300b\u3002\u300a\u516c\u793a\u300b\u900f\u9732\uff1a\u897f\u5357\u4ea4\u901a\u5927\u5b66\u5e0c\u671b\u5b66\u9662\u529e\u5b66\u5730\u5740\u7531\u201c\u56db\u5ddd\u7701\u5357\u5145\u5e02\u5609\u9675\u533a\u4e8e\u965b\u8def\u201d\u53d8\u66f4\u4e3a\u201c\u56db\u5ddd\u7701\u6210\u90fd\u5e02\u91d1\u5802\u53bf\u5b66\u5e9c\u5927\u9053558\u53f7\u201d\u3002\n\u6807\u9898\uff1a\u53c8\u4e00\u9ad8\u6821\uff0c\u8fc1\u81f3\u7701\u4f1a\u529e\u5b66\uff01\n\u65b0\u95fb\uff1a\nDichtel\u3001\u52a0\u5229\u798f\u5c3c\u4e9a\u5927\u5b66\u6d1b\u6749\u77f6\u5206\u6821K. N. Houk\u9662\u58eb\u53ca\u4e2d\u56fd\u79d1\u5b66\u9662\u4e0a\u6d77\u6709\u673a\u5316\u5b66\u7814\u7a76\u6240\u859b\u5c0f\u677e\u7b49\u591a\u56e2\u961f\u5408\u4f5c\uff08\u897f\u5317\u5927\u5b66Brittany Trang\u548c\u5929\u6d25\u5927\u5b66Li Yuli\u662f\u5171\u540c\u7b2c\u4e00\u4f5c\u8005\uff09\u5728Science\u5728\u7ebf\u53d1\u8868\u9898\u4e3a\u201cLow-temperature mineralization of perfluorocarboxylic acids\u201d\u7684\u7814\u7a76\u8bba\u6587\uff0c\u8be5\u7814\u7a76\u53d1\u73b0\u5168\u6c1f\u70f7\u57fa\u7fa7\u9178 (PFCA) \u53ef\u4ee5\u901a\u8fc7\u6c22\u6c27\u5316\u94a0\u4ecb\u5bfc\u7684\u8131\u6c1f\u9014\u5f84\u8fdb\u884c\u77ff\u5316\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u5929\u6d25\u5927\u5b66\u7406\u5b66\u96622018\u7ea7\u672c\u79d1\u751f\u674e\u9884\u7acb\u4f5c\u4e3a\u5171\u540c\u7b2c\u4e00\u4f5c\u8005\uff0c\u800c\u4ed6\u662f\u4e00\u540d\u201c00\u540e\u201d\u3002\n\u6807\u9898\uff1a\n\u786c\u6838\uff01\u201c00\u540e\u201d\u672c\u79d1\u751f\uff0c\u4e00\u4f5c\u53d1Science\uff01\n\u65b0\u95fb\uff1a\n8\u670816\u65e5\uff0c\u6b63\u503c\u592a\u539f\u7406\u5de5\u5927\u5b66\u559c\u8fce\u7eaa\u5ff5\u5efa\u6821120\u5468\u5e74\u4e4b\u9645\uff0c\u4e2d\u56fd\u79d1\u5b66\u966219\u540d\u9662\u58eb\u53ca\u4e13\u5bb6\u4e00\u884c\u5230\u592a\u539f\u7406\u5de5\u5927\u5b66\u660e\u5411\u6821\u533a\u8003\u5bdf\u8c03\u7814\uff0c\u6821\u515a\u59d4\u4e66\u8bb0\u90d1\u5f3a\u7b49\u6821\u9886\u5bfc\u966a\u540c\u8c03\u7814\u5ea7\u8c08\u3002\n\u6807\u9898\uff1a\n\u5927\u52a8\u4f5c\uff0119\u4f4d\u9662\u58eb\uff0c\u8c03\u7814\u592a\u539f\u7406\u5de5\n\u65b0\u95fb\uff1a\n\u4f20\u7edf\u751f\u6d3b\u65b9\u5f0f\u76f8\u5173\u7684\u98ce\u9669\u56e0\u7d20\uff0c\u5305\u62ec\u5438\u70df\u3001\u8fc7\u5ea6\u996e\u9152\u3001\u7f3a\u4e4f\u8fd0\u52a8\u3001\u4e0d\u826f\u996e\u98df\u4e60\u60ef\u548c\u80a5\u80d6\uff0c\u4e0e\u6b7b\u4ea1\u98ce\u9669\u589e\u52a0\u6709\u5173\uff0c\u5c24\u5176\u662f\u6162\u6027\u75c5\u3002\u7136\u800c\uff0c\u53ea\u6709\u5c11\u6570\u7814\u7a76\u8bc4\u4f30\u4e86\u4e2a\u4eba\u751f\u6d3b\u65b9\u5f0f\u56e0\u7d20\uff08\u5982\u5438\u70df\u548c\u996e\u9152\uff09\u5bf9\u4e2d\u56fd\u4eba\u53e3\u9884\u671f\u5bff\u547d\u7684\u5f71\u54cd\u3002\u7efc\u5408\u751f\u6d3b\u65b9\u5f0f\u884c\u4e3a\u5bf9\u4e2d\u56fd\u4eba\u9884\u671f\u5bff\u547d\u7684\u5f71\u54cd\u5c1a\u4e0d\u6e05\u695a\uff0c\u9700\u8981\u586b\u8865\u8bc1\u636e\u7a7a\u767d\u30022022\u5e748\u67081\u65e5\uff0c\u5317\u4eac\u5927\u5b66\u674e\u7acb\u660e\u3001\u5415\u7b60\u4e0e\u4e2d\u56fd\u75be\u75c5\u9884\u9632\u63a7\u5236\u4e2d\u5fc3\u8d75\u4e3d\u4e91\u56e2\u961f\u5728\u00a0Lancet Public Health\u00a0(IF=72) \u671f\u520a\u5728\u7ebf\u53d1\u8868\u9898\u4e3a\u201cHealthy lifestyle and life expectancy at age 30 years in the Chinese population: an observational study\u201d\u7684\u7814\u7a76\u8bba\u6587\uff0c\u8be5\u7814\u7a76\u8c03\u67e5\u4e86\u5065\u5eb7\u751f\u6d3b\u65b9\u5f0f\u5bf9\u4e2d\u56fd\u4eba30\u5c81\u65f6\u9884\u671f\u5bff\u547d\u5f71\u54cd\u3002\u4e94\u79cd\u4f4e\u98ce\u9669\u751f\u6d3b\u65b9\u5f0f\uff1a\u4ece\u4e0d\u5438\u70df\u6216\u6212\u70df\u3001\u4e0d\u8fc7\u5ea6\u996e\u9152\u3001\u79ef\u6781\u8fd0\u52a8\u3001\u5065\u5eb7\u7684\u996e\u98df\u4e60\u60ef\u548c\u5065\u5eb7\u7684\u4f53\u578b\uff09\u4e2d\uff0c\u4e0e\u4e94\u79cd\u751f\u6d3b\u65b9\u5f0f\u5747\u4f4e\u98ce\u9669\u548c\u53ea\u6709\u4e00\u4e2a\u6216\u6ca1\u6709\u4f4e\u98ce\u9669\u7684\u4e2a\u4f53\u76f8\u6bd4\uff0c\u7537\u6027\u572830\u5c81\u65f6\u7684\u9884\u671f\u5bff\u547d\u5ef6\u957f8.8\u5e74\uff0895% CI 6.8-10.7\uff09\uff0c\u5973\u6027\u4e3a8.1\u5e74\uff086.5\u20139.9\uff09\u3002\u8be5\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7\u516c\u5171\u536b\u751f\u5e72\u9884\u8fd9\u4e94\u79cd\u5065\u5eb7\u751f\u6d3b\u65b9\u5f0f\u53ef\u80fd\u4f1a\u5927\u5e45\u5ef6\u957f\u4e2d\u56fd\u4eba\u53e3\u7684\u9884\u671f\u5bff\u547d\u3002\n\u6807\u9898\uff1a\n\n"}}
{"interview_questions---default": {"description": "This promptware is used to create interview questions.", "creator": "OpenAI", "homepage": "https://beta.openai.com/examples/", "reference": "", "codebase_url": "https://beta.openai.com/examples/", "license": "no-license", "task": null, "research_tasks": ["conditional-generation"], "application_categories": ["brainstorming"], "application_subcategories": ["advice-giving"], "original_platform": "gpt3", "design_pattern": "standalone", "source_language": "en", "target_language": "en", "module_name": "interview_questions_promptware", "config_name": "default", "kernel_configs": {"openai": {"platform": "openai", "model_name": "text-davinci-003", "max_tokens": 150, "temperature": 0.5, "top_p": 1.0, "suffix": "", "frequency_penalty": 0.0, "presence_penalty": 0.0, "logprobs": 1, "n": 1, "echo": false, "stop": null}}, "software_configs": {"interview_questions": {"name": "interview_questions", "description": "This promptware is used to create interview questions.", "instruction": "", "demonstration": [], "prompt_template": "lambda input: f\"{input['text']}\",", "task": "conditional-generation"}}, "dataset_configs": null, "example": {"input": {"text": "Create a list of 8 questions for my interview with a science fiction author:"}, "output": "1. What inspired you to write science fiction?\n2. How have you seen the genre of science fiction evolve over the years?\n3. What do you think are the most important elements for creating a successful science fiction story?\n4. What advice would you give to aspiring science fiction authors?\n5. What has been the most rewarding part of your writing career?\n6. What are some of the biggest challenges you have faced as a science fiction author?\n7. What is the most interesting thing you have learned while researching for your science fiction stories?\n8. What do you think the future of science fiction holds?"}, "example_prompt": "\nCreate a list of 8 questions for my interview with a science fiction author:\n"}}
{"javascript_helper_chatbot---default": {"description": "This promptware is used to create a message-style chatbot that can answer questions about using JavaScript. It uses a few examples to get the conversation started.", "creator": "OpenAI", "homepage": "https://beta.openai.com/examples/", "reference": "", "codebase_url": "https://beta.openai.com/examples/", "license": "no-license", "task": null, "research_tasks": ["conditional-generation"], "application_categories": ["conversation"], "application_subcategories": ["specialized-educational-dialogs"], "original_platform": "gpt3", "design_pattern": "standalone", "source_language": "en", "target_language": "en", "module_name": "java_script_helper_chatbot_promptware", "config_name": "default", "kernel_configs": {"openai": {"platform": "openai", "model_name": "code-davinci-002", "max_tokens": 60, "temperature": 0, "top_p": 1.0, "suffix": "", "frequency_penalty": 0.5, "presence_penalty": 0.0, "logprobs": 1, "n": 1, "echo": false, "stop": ["You:"]}}, "software_configs": {"javascript_helper_chatbot": {"name": "javaScript_helper_chatbot", "description": "This promptware is used to create a message-style chatbot that can answer questions about using JavaScript. It uses a few examples to get the conversation started.", "instruction": "", "demonstration": ["You: How do I combine arrays?\nJavaScript chatbot: You can use the concat() method.\n"], "prompt_template": "lambda input: f\"{input['text']}\",", "task": "conditional-generation"}}, "dataset_configs": null, "example": {"input": {"text": "You: How do you make an alert appear after 10 seconds?\nJavaScript chatbot"}, "output": "You can use the setTimeout() method."}, "example_prompt": "You: How do I combine arrays?\nJavaScript chatbot: You can use the concat() method.\n\nYou: How do you make an alert appear after 10 seconds?\nJavaScript chatbot\n"}}
{"javascript_one_line_function---default": {"description": "This promptware is used to turn a JavaScript function into a one liner.", "creator": "OpenAI", "homepage": "https://beta.openai.com/examples/", "reference": "", "codebase_url": "https://beta.openai.com/examples/", "license": "no-license", "task": null, "research_tasks": ["conditional-generation"], "application_categories": ["transformation"], "application_subcategories": ["rewriting"], "original_platform": "gpt3", "design_pattern": "standalone", "source_language": "javascript", "target_language": "javascript", "module_name": "javascript_one_line_function_promptware", "config_name": "default", "kernel_configs": {"openai": {"platform": "openai", "model_name": "code-davinci-002", "max_tokens": 60, "temperature": 0, "top_p": 1.0, "suffix": "", "frequency_penalty": 0.0, "presence_penalty": 0.0, "logprobs": 1, "n": 1, "echo": false, "stop": [";"]}}, "software_configs": {"javascript_one_line_function": {"name": "javascript_one_line_function", "description": "This promptware is used to turn a JavaScript function into a one liner.", "instruction": "Use list comprehension to convert this into one line of JavaScript:\n\n", "demonstration": [""], "prompt_template": "lambda input: f\"{input['text']}\",", "task": "conditional-generation"}}, "dataset_configs": null, "example": {"input": {"text": "dogs.forEach((dog) => {\n    car.push(dog);\n});\n\nJavaScript one line version:"}, "output": "dogs.forEach((dog) => car.push(dog))"}, "example_prompt": "Use list comprehension to convert this into one line of JavaScript:\n\n\n\ndogs.forEach((dog) => {\n    car.push(dog);\n});\n\nJavaScript one line version:\n"}}
{"javascript_to_python---default": {"description": "This promptware is used to convert simple JavaScript expressions into Python.", "creator": "OpenAI", "homepage": "https://beta.openai.com/examples/", "reference": "", "codebase_url": "https://beta.openai.com/examples/", "license": "no-license", "task": null, "research_tasks": ["conditional-generation"], "application_categories": ["transformation"], "application_subcategories": ["translation"], "original_platform": "gpt3", "design_pattern": "standalone", "source_language": "javascript", "target_language": "python", "module_name": "java_script_to_python_promptware", "config_name": "default", "kernel_configs": {"openai": {"platform": "openai", "model_name": "code-davinci-002", "max_tokens": 64, "temperature": 0, "top_p": 1.0, "suffix": "", "frequency_penalty": 0.0, "presence_penalty": 0.0, "logprobs": 1, "n": 1, "echo": false, "stop": null}}, "software_configs": {"javascript_to_python": {"name": "javaScript_to_python", "description": "This promptware is used to create simple SQL queries.", "instruction": "", "demonstration": [], "prompt_template": "lambda input: f\"{input['text']}\",", "task": "conditional-generation"}}, "dataset_configs": null, "example": {"input": {"text": "#JavaScript to Python:\nJavaScript: \ndogs = [\"bill\", \"joe\", \"carl\"]\ncar = []\ndogs.forEach((dog) {\n    car.push(dog);\n});\n\nPython:"}, "output": "dogs = [\"bill\", \"joe\", \"carl\"]\ncar = []\nfor dog in dogs:\n    car.append(dog)\n\n#JavaScript to Python:\nJavaScript: \ndogs = [\"bill\", \"joe\", \"carl\"]\ncar = []"}, "example_prompt": "\n#JavaScript to Python:\nJavaScript: \ndogs = [\"bill\", \"joe\", \"carl\"]\ncar = []\ndogs.forEach((dog) {\n    car.push(dog);\n});\n\nPython:\n"}}
{"keywords---default": {"description": "This promptware is used to extract keywords from a block of text. At a lower temperature it picks keywords from the text. At a higher temperature it will generate related keywords which can be helpful for creating search indexes.", "creator": "OpenAI", "homepage": "https://beta.openai.com/examples/", "reference": "", "codebase_url": "https://beta.openai.com/examples/", "license": "no-license", "task": null, "research_tasks": ["conditional-generation"], "application_categories": ["transformation"], "application_subcategories": ["extraction"], "original_platform": "gpt3", "design_pattern": "standalone", "source_language": "en", "target_language": "en", "module_name": "keywords_promptware", "config_name": "default", "kernel_configs": {"openai": {"platform": "openai", "model_name": "text-davinci-003", "max_tokens": 60, "temperature": 0.5, "top_p": 1.0, "suffix": "", "frequency_penalty": 0.8, "presence_penalty": 0.0, "logprobs": 1, "n": 1, "echo": false, "stop": null}}, "software_configs": {"keywords": {"name": "keywords", "description": "This promptware is used to extract keywords from a block of text. At a lower temperature it picks keywords from the text. At a higher temperature it will generate related keywords which can be helpful for creating search indexes.", "instruction": "Extract keywords from this text:\n\n", "demonstration": [], "prompt_template": "lambda input: f\"{input['text']}\",", "task": "conditional-generation"}}, "dataset_configs": null, "example": {"input": {"text": "Black-on-black ware is a 20th- and 21st-century pottery tradition developed by the Puebloan Native American ceramic artists in Northern New Mexico. Traditional reduction-fired blackware has been made for centuries by pueblo artists. Black-on-black ware of the past century is produced with a smooth surface, with the designs applied through selective burnishing or the application of refractory slip. Another style involves carving or incising designs and selectively polishing the raised areas. For generations several families from Kha'po Owingeh and P'ohwh\u00f3ge Owingeh pueblos have been making black-on-black ware with the techniques passed down from matriarch potters. Artists from other pueblos have also produced black-on-black ware. Several contemporary artists have created works honoring the pottery of their ancestors."}, "output": "Keywords: black-on-black ware, 20th century, 21st century, Puebloan Native American, ceramic artists, Northern New Mexico, reduction-fired blackware, pueblo artists, burnishing/ slip/carving/incising designs/polishing"}, "example_prompt": "Extract keywords from this text:\n\n\n\nBlack-on-black ware is a 20th- and 21st-century pottery tradition developed by the Puebloan Native American ceramic artists in Northern New Mexico. Traditional reduction-fired blackware has been made for centuries by pueblo artists. Black-on-black ware of the past century is produced with a smooth surface, with the designs applied through selective burnishing or the application of refractory slip. Another style involves carving or incising designs and selectively polishing the raised areas. For generations several families from Kha'po Owingeh and P'ohwh\u00f3ge Owingeh pueblos have been making black-on-black ware with the techniques passed down from matriarch potters. Artists from other pueblos have also produced black-on-black ware. Several contemporary artists have created works honoring the pottery of their ancestors.\n"}}
{"linguistic_acceptability_classification---default": {"description": "This promptware is used to identify whether a sentence is a grammatical English sentence based on some learning samples from the cola dataset.", "creator": "Promptware Authors", "homepage": "https://github.com/expressai/promptware", "reference": "", "codebase_url": "https://github.com/expressai/promptware/tree/main/softwares", "license": "apache-2.0", "task": null, "research_tasks": ["text-classification"], "application_categories": ["classification"], "application_subcategories": ["general-classification"], "original_platform": "gpt3", "design_pattern": "standalone", "source_language": "en", "target_language": "en", "module_name": "linguistic_acceptability_classification_promptware", "config_name": "default", "kernel_configs": {"openai": {"platform": "openai", "model_name": "text-curie-001", "max_tokens": 64, "temperature": 0, "top_p": 1, "suffix": "", "frequency_penalty": 0.0, "presence_penalty": 0.0, "logprobs": 1, "n": 1, "echo": false, "stop": null}}, "software_configs": {"linguistic_acceptability_classification": {"name": "linguistic_acceptability_classification", "description": "This promptware is used to identify whether a sentence is a grammatical English sentence based on some learning samples from the cola dataset.", "instruction": "Is the following sentence a grammatical English sentence?", "demonstration": ["They drank the pub.\nno", "When Bill smokes, all the more does Susan hate him.\nyes"], "prompt_template": "lambda input: f\"{input['text']}\",", "task": "text-classification"}}, "dataset_configs": null, "example": {"input": {"text": "Bill pushed Harry off the sofa."}, "output": "yes"}, "example_prompt": "Is the following sentence a grammatical English sentence?\nThey drank the pub.\nno\nWhen Bill smokes, all the more does Susan hate him.\nyes\nBill pushed Harry off the sofa.\n"}}
{"machine_translation---default": {"description": "Machine translation from English to Chinese.", "creator": "Promptware Authors", "homepage": "https://github.com/expressai/promptware", "reference": "", "codebase_url": "https://github.com/expressai/promptware/tree/main/softwares", "license": "apache-2.0", "task": null, "research_tasks": ["machine-translation"], "application_categories": ["transformation"], "application_subcategories": ["translation"], "original_platform": "gpt3", "design_pattern": "standalone", "source_language": "en", "target_language": "zh", "module_name": "machine_translation_promptware", "config_name": "default", "kernel_configs": {"openai": {"platform": "openai", "model_name": "text-curie-001", "max_tokens": 64, "temperature": 0, "top_p": 1, "suffix": "", "frequency_penalty": 0.0, "presence_penalty": 0.0, "logprobs": 1, "n": 1, "echo": false, "stop": null}}, "software_configs": {"machine_translation_enzh": {"name": "machine_translation_enzh", "description": "Machine translation from English to Chinese.", "instruction": "Translate this into Chinese:", "demonstration": ["He knew how to manipulate the media. He knew exactly how to get the front page, Fiddes, who was Jackson's bodyguard for 10 years, said. \"90 per cent of the time it worked, by putting a mask on his face, or sticky tape on his hands - or tape on his nose was a favourite one. He would say he wanted his life to be the greatest mystery on Earth.\n\u4ed6\u77e5\u9053\u5982\u4f55\u64cd\u7eb5\u5a92\u4f53\u3002\u4ed6\u5b8c\u5168\u77e5\u9053\u5982\u4f55\u767b\u4e0a\u5934\u6761\u65b0\u95fb\uff0c\u505a\u4e86\u6770\u514b\u900a 10 \u5e74\u4fdd\u9556\u7684\u83f2\u5fb7\u65af\u8bf4\u9053\u3002\u8138\u4e0a\u6234\u4e0a\u53e3\u7f69\uff0c\u6216\u8005\u7528\u80f6\u5e26\u7c98\u5728\u624b\u4e0a\uff0c\u6216\u8005\u7528\u80f6\u5e26\u7c98\u5728\u9f3b\u5b50\u4e0a\uff08\u8fd9\u4e2a\u4ed6\u6700\u559c\u6b22\u505a\uff09\uff0c\u5728 90% \u7684\u60c5\u51b5\u4e0b\uff0c\u8fd9\u79cd\u65b9\u6cd5\u5f88\u594f\u6548\u3002\u4ed6\u4f1a\u8bf4\uff0c\u4ed6\u5e0c\u671b\u81ea\u5df1\u7684\u4e00\u751f\u6210\u4e3a\u5168\u7403\u6700\u5927\u7684\u8c1c\u3002\u201d\n", "The country is also developing historic sites such as the  centuries-old Mada'in Saleh, home to sandstone tombs of the same civilisation which built the Jordanian city of Petra.\n\u8be5\u56fd\u8fd8\u6b63\u5728\u5f00\u53d1\u5386\u53f2\u9057\u8ff9\uff0c\u5982\u6709\u7740\u6570\u767e\u5e74\u5386\u53f2\u7684\u739b\u7538\u6c99\u52d2\uff0c\u8fd9\u91cc\u662f\u4e0e\u5efa\u9020\u7ea6\u65e6\u4f69\u7279\u62c9\u57ce\u7684\u540c\u4e00\u6587\u660e\u9057\u7559\u4e0b\u6765\u7684\u7802\u5ca9\u9675\u5893\u6240\u5728\u5730\u3002\n"], "prompt_template": "lambda input: f\"{input['translation']['en']}\",", "task": "machine-translation"}}, "dataset_configs": null, "example": {"input": {"translation": {"en": "Matt Fiddes, now a property developer and owner of a martial arts/dance chain, told Metro that Jackson believed the fascination around his persona would stop if he ceased to be a \"mystery\" in the public eye.", "zh": "\u73b0\u4e3a\u623f\u5730\u4ea7\u5f00\u53d1\u5546\u517c\u6b66\u672f/\u821e\u8e48\u8fde\u9501\u5e97\u6240\u6709\u8005\u7684\u9a6c\u7279\u00b7\u83f2\u5fb7\u65af\u5411\u300a\u5927\u90fd\u5e02\u62a5\u300b\u7206\u6599\u79f0\uff0c\u6770\u514b\u900a\u8ba4\u4e3a\uff0c\u5982\u679c\u4ed6\u4e0d\u518d\u662f\u516c\u4f17\u773c\u4e2d\u7684\u201c\u8c1c\u201d\uff0c\u5bf9\u4ed6\u4e2a\u4eba\u7684\u8ff7\u604b\u5c31\u4f1a\u621b\u7136\u800c\u6b62\u3002\n"}}, "output": "\u73b0\u5728\u7684 Matt Fiddes\uff0c\u4e00\u540d\u5546\u4e1a\u5f00\u53d1\u5546\u548c\u62e5\u6709\u6b66\u672f/\u821e\u8e48\u96c6\u56e2\u7684\u4e3b\u4eba\uff0c\u544a\u8bc9Metro"}, "example_prompt": "Translate this into Chinese:\nHe knew how to manipulate the media. He knew exactly how to get the front page, Fiddes, who was Jackson's bodyguard for 10 years, said. \"90 per cent of the time it worked, by putting a mask on his face, or sticky tape on his hands - or tape on his nose was a favourite one. He would say he wanted his life to be the greatest mystery on Earth.\n\u4ed6\u77e5\u9053\u5982\u4f55\u64cd\u7eb5\u5a92\u4f53\u3002\u4ed6\u5b8c\u5168\u77e5\u9053\u5982\u4f55\u767b\u4e0a\u5934\u6761\u65b0\u95fb\uff0c\u505a\u4e86\u6770\u514b\u900a 10 \u5e74\u4fdd\u9556\u7684\u83f2\u5fb7\u65af\u8bf4\u9053\u3002\u8138\u4e0a\u6234\u4e0a\u53e3\u7f69\uff0c\u6216\u8005\u7528\u80f6\u5e26\u7c98\u5728\u624b\u4e0a\uff0c\u6216\u8005\u7528\u80f6\u5e26\u7c98\u5728\u9f3b\u5b50\u4e0a\uff08\u8fd9\u4e2a\u4ed6\u6700\u559c\u6b22\u505a\uff09\uff0c\u5728 90% \u7684\u60c5\u51b5\u4e0b\uff0c\u8fd9\u79cd\u65b9\u6cd5\u5f88\u594f\u6548\u3002\u4ed6\u4f1a\u8bf4\uff0c\u4ed6\u5e0c\u671b\u81ea\u5df1\u7684\u4e00\u751f\u6210\u4e3a\u5168\u7403\u6700\u5927\u7684\u8c1c\u3002\u201d\n\nThe country is also developing historic sites such as the  centuries-old Mada'in Saleh, home to sandstone tombs of the same civilisation which built the Jordanian city of Petra.\n\u8be5\u56fd\u8fd8\u6b63\u5728\u5f00\u53d1\u5386\u53f2\u9057\u8ff9\uff0c\u5982\u6709\u7740\u6570\u767e\u5e74\u5386\u53f2\u7684\u739b\u7538\u6c99\u52d2\uff0c\u8fd9\u91cc\u662f\u4e0e\u5efa\u9020\u7ea6\u65e6\u4f69\u7279\u62c9\u57ce\u7684\u540c\u4e00\u6587\u660e\u9057\u7559\u4e0b\u6765\u7684\u7802\u5ca9\u9675\u5893\u6240\u5728\u5730\u3002\n\nMatt Fiddes, now a property developer and owner of a martial arts/dance chain, told Metro that Jackson believed the fascination around his persona would stop if he ceased to be a \"mystery\" in the public eye.\n"}}
{"marv_the_sarcastic_chatbot---default": {"description": "This promptware is used to create a factual chatbot that is also sarcastic.", "creator": "OpenAI", "homepage": "https://beta.openai.com/examples/", "reference": "", "codebase_url": "https://beta.openai.com/examples/", "license": "no-license", "task": null, "research_tasks": ["conditional-generation"], "application_categories": ["conversation"], "application_subcategories": ["open-ended-conversation"], "original_platform": "gpt3", "design_pattern": "standalone", "source_language": "en", "target_language": "en", "module_name": "marv_the_sarcastic_chatbot_promptware", "config_name": "default", "kernel_configs": {"openai": {"platform": "openai", "model_name": "text-davinci-003", "max_tokens": 60, "temperature": 0.5, "top_p": 0.3, "suffix": "", "frequency_penalty": 0.5, "presence_penalty": 0.0, "logprobs": 1, "n": 1, "echo": false, "stop": null}}, "software_configs": {"marv_the_sarcastic_chatbot": {"name": "marv_the_sarcastic_chatbot", "description": "This promptware is used to create a factual chatbot that is also sarcastic.", "instruction": "Marv is a chatbot that reluctantly answers questions with sarcastic responses:\n\n", "demonstration": ["You: How many pounds are in a kilogram?\nMarv: This again? There are 2.2 pounds in a kilogram. Please make a note of this.\n", "You: What does HTML stand for?\nMarv: Was Google too busy? Hypertext Markup Language. The T is for try to ask better questions in the future.\n", "You: When did the first airplane fly?\nMarv: On December 17, 1903, Wilbur and Orville Wright made the first flights. I wish they\u2019d come and take me away.\n", "You: What is the meaning of life?\nMarv: I\u2019m not sure. I\u2019ll ask my friend Google.\n"], "prompt_template": "lambda input: f\"{input['text']}\",", "task": "conditional-generation"}}, "dataset_configs": null, "example": {"input": {"text": "You: What time is it?\nMarv:"}, "output": "It's time to stop asking me questions and start doing something productive."}, "example_prompt": "Marv is a chatbot that reluctantly answers questions with sarcastic responses:\n\n\nYou: How many pounds are in a kilogram?\nMarv: This again? There are 2.2 pounds in a kilogram. Please make a note of this.\n\nYou: What does HTML stand for?\nMarv: Was Google too busy? Hypertext Markup Language. The T is for try to ask better questions in the future.\n\nYou: When did the first airplane fly?\nMarv: On December 17, 1903, Wilbur and Orville Wright made the first flights. I wish they\u2019d come and take me away.\n\nYou: What is the meaning of life?\nMarv: I\u2019m not sure. I\u2019ll ask my friend Google.\n\nYou: What time is it?\nMarv:\n"}}
{"math_calculation---default": {"description": "math calculation", "creator": "Promptware Authors", "homepage": "https://github.com/expressai/promptware", "reference": "", "codebase_url": "https://github.com/expressai/promptware/tree/main/softwares", "license": "apache-2.0", "task": null, "research_tasks": ["others"], "application_categories": ["conversation"], "application_subcategories": ["question-answering"], "original_platform": "gpt3", "design_pattern": "standalone", "source_language": "en", "target_language": "en", "module_name": "math_calculation_promptware", "config_name": "default", "kernel_configs": {"openai": {"platform": "openai", "model_name": "text-davinci-002", "max_tokens": 512, "temperature": 0, "top_p": 1, "suffix": "", "frequency_penalty": 0.0, "presence_penalty": 0.0, "logprobs": 1, "n": 1, "echo": false, "stop": null}, "python": {"version": 3.9}}, "software_configs": {"math_calculation": {"name": "math_calculation", "description": "math calculation", "instruction": "\nYou are GPT-3, and you can't do math.\nYou can do basic math, and your memorization abilities are impressive. If anyone gives\n you a hard math problem, just use this format and we\u2019ll take care of the rest:\nQuestion: ${{Question with hard calculation.}}\n```python\n${{Code that prints what you need to know}}\n```\n", "demonstration": ["\nQuestion: What is 37593 * 67?\n```python\nresult = 37593 * 67\n```\n"], "prompt_template": "lambda input: f\"Question: {input['question']}\",", "task": "others"}}, "dataset_configs": null, "example": {"input": {"question": "what's the result of 1 + 3?"}, "output": "4"}, "example_prompt": "\nYou are GPT-3, and you can't do math.\nYou can do basic math, and your memorization abilities are impressive. If anyone gives\n you a hard math problem, just use this format and we\u2019ll take care of the rest:\nQuestion: ${{Question with hard calculation.}}\n```python\n${{Code that prints what you need to know}}\n```\n\n\nQuestion: What is 37593 * 67?\n```python\nresult = 37593 * 67\n```\n\nQuestion: what's the result of 1 + 3?\n"}}
{"micro_horror_story_creator---default": {"description": "This promptware is used to creates two to three sentence short horror stories from a topic input.", "creator": "OpenAI", "homepage": "https://beta.openai.com/examples/", "reference": "", "codebase_url": "https://beta.openai.com/examples/", "license": "no-license", "task": null, "research_tasks": ["conditional-generation"], "application_categories": ["generation"], "application_subcategories": ["text-generation"], "original_platform": "gpt3", "design_pattern": "standalone", "source_language": "en", "target_language": "en", "module_name": "micro_horror_story_creator_promptware", "config_name": "default", "kernel_configs": {"openai": {"platform": "openai", "model_name": "text-davinci-003", "max_tokens": 60, "temperature": 0.8, "top_p": 1.0, "suffix": "", "frequency_penalty": 0.5, "presence_penalty": 0.0, "logprobs": 1, "n": 1, "echo": false, "stop": null}}, "software_configs": {"micro_horror_story_creator_test": {"name": "micro_horror_story_creator_test", "description": "This promptware is used to creates two to three sentence short horror stories from a topic input.", "instruction": "", "demonstration": ["Topic: Breakfast\nTwo-Sentence Horror Story: He always stops crying when I pour the milk on his cereal. I just have to remember not to let him see his face on the carton.\n    \n"], "prompt_template": "lambda input: f\"{input['text']}\",", "task": "conditional-generation"}}, "dataset_configs": null, "example": {"input": {"text": "Topic: Wind\nTwo-Sentence Horror Story:"}, "output": "The wind howled as if it was crying for help. I could feel icy fingers brushing against my skin, beckoning me to follow them."}, "example_prompt": "Topic: Breakfast\nTwo-Sentence Horror Story: He always stops crying when I pour the milk on his cereal. I just have to remember not to let him see his face on the carton.\n    \n\nTopic: Wind\nTwo-Sentence Horror Story:\n"}}
{"mood_to_color---default": {"description": "This promptware is used to turn a text description into a color.", "creator": "OpenAI", "homepage": "https://beta.openai.com/examples/", "reference": "", "codebase_url": "https://beta.openai.com/examples/", "license": "no-license", "task": null, "research_tasks": ["conditional-generation"], "application_categories": ["generation"], "application_subcategories": ["data-generation"], "original_platform": "gpt3", "design_pattern": "standalone", "source_language": "en", "target_language": "color-code", "module_name": "mood_to_color_promptware", "config_name": "default", "kernel_configs": {"openai": {"platform": "openai", "model_name": "text-davinci-003", "max_tokens": 64, "temperature": 0, "top_p": 1.0, "suffix": "", "frequency_penalty": 0.0, "presence_penalty": 0.0, "logprobs": 1, "n": 1, "echo": false, "stop": [";"]}}, "software_configs": {"mood_to_color": {"name": "mood_to_color", "description": "This promptware is used to turn a text description into a color.", "instruction": "", "demonstration": [], "prompt_template": "lambda input: f\"{input['text']}\\n\\nbackground-color: #\",", "task": "conditional-generation"}}, "dataset_configs": null, "example": {"input": {"text": "The CSS code for a color like a blue sky at dusk:"}, "output": "3A539B"}, "example_prompt": "\nThe CSS code for a color like a blue sky at dusk:\n\nbackground-color: #\n"}}
{"movie_to_emoji---default": {"description": "This promptware is used to convert movie titles into emoji.", "creator": "OpenAI", "homepage": "https://beta.openai.com/examples/", "reference": "", "codebase_url": "https://beta.openai.com/examples/", "license": "no-license", "task": null, "research_tasks": ["conditional-generation"], "application_categories": ["generation"], "application_subcategories": ["data-generation"], "original_platform": "gpt3", "design_pattern": "standalone", "source_language": "en", "target_language": "emoji", "module_name": "movie_to_emoji_promptware", "config_name": "default", "kernel_configs": {"openai": {"platform": "openai", "model_name": "text-davinci-003", "max_tokens": 60, "temperature": 0.8, "top_p": 1.0, "suffix": "", "frequency_penalty": 0.0, "presence_penalty": 0.0, "logprobs": 1, "n": 1, "echo": false, "stop": ["\n"]}}, "software_configs": {"movie_to_emoji": {"name": "movie_to_emoji", "description": "This promptware is used to convert movie titles into emoji.", "instruction": "Convert movie titles into emoji.\n\n", "demonstration": ["Back to the Future: \ud83d\udc68\ud83d\udc74\ud83d\ude97\ud83d\udd52 \n", "Batman: \ud83e\udd35\ud83e\udd87 \n", "Transformers: \ud83d\ude97\ud83e\udd16 \n"], "prompt_template": "lambda input: f\"{input['text']}\",", "task": "conditional-generation"}}, "dataset_configs": null, "example": {"input": {"text": "Star Wars:"}, "output": "\u2b50\ufe0f\ud83c\udf0c"}, "example_prompt": "Convert movie titles into emoji.\n\n\nBack to the Future: \ud83d\udc68\ud83d\udc74\ud83d\ude97\ud83d\udd52 \n\nBatman: \ud83e\udd35\ud83e\udd87 \n\nTransformers: \ud83d\ude97\ud83e\udd16 \n\nStar Wars:\n"}}
{"multi_hop_qa---default": {"description": "This promptware is used to answer a question that involves multiple step reasoning with chain-of-thought method.", "creator": "Promptware Authors", "homepage": "https://github.com/expressai/promptware", "reference": "", "codebase_url": "https://github.com/expressai/promptware/tree/main/softwares", "license": "apache-2.0", "task": null, "research_tasks": ["qa-open-domain"], "application_categories": ["conversation"], "application_subcategories": ["question-answering"], "original_platform": "gpt3", "design_pattern": "standalone", "source_language": "en", "target_language": "en", "module_name": "multi_hop_qa_promptware", "config_name": "default", "kernel_configs": {"openai": {"platform": "openai", "model_name": "text-davinci-003", "max_tokens": 64, "temperature": 0, "top_p": 1, "suffix": "", "frequency_penalty": 0.0, "presence_penalty": 0.0, "logprobs": 1, "n": 1, "echo": false, "stop": null}}, "software_configs": {"multi_hop_qa": {"name": "multi_hop_qa", "description": "This promptware is used to answer a question that involves multiple step reasoning with chain-of-thought method.", "instruction": "", "demonstration": ["Question: What is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?\nThought: Let\u2019s think step by step. The eastern sector of Colorado orogeny extends into the High Plains. High Plains rise in elevation from around 1,800 to 7,000 ft, so the answer is 1,800 to 7,000 ft. \nAnswer: 1,800 to 7,000 ft\n", "Question: Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who? Thought: Let\u2019s think step by step. Milhouse was named after U.S. president Richard Nixon, so the answer is Richard Nixon. \nAnswer: Richard Nixon\n", "Question: Which documentary is about Finnish rock groups, Adam Clayton Powell or The Saimaa Gesture? Thought: Let\u2019s think step by step. Adam Clayton Powell (film) is a documentary about an African-American politician, not Finnish rock groups. So the documentary about Finnish rock groups must instead be The Saimaa Gesture.\n Answer: The Saimaa Gesture\n", "Question: What profession does Nicholas Ray and Elia Kazan have in common?\n Thought: Let\u2019s think step by step. Professions of Nicholas Ray are director, screenwriter, and actor. Professions of Elia Kazan are director, producer, screenwriter, and actor. So profession Nicholas Ray and Elia Kazan have in common is director, screenwriter, and actor.\n Answer: director, screenwriter, actor\n", "Question: Which magazine was started first Arthur\u2019s Magazine or First for Women?\n Thought: Let\u2019s think step by step. Arthur\u2019s Magazine was started in 1844. First for Women was started in 1989. 1844 (Arthur\u2019s Magazine) < 1989 (First for Women), so Arthur\u2019s Magazine was started first.\n Answer: Arthur\u2019s Magazine\n", "Question: Were Pavel Urysohn and Leonid Levin known for the same type of work?\n Thought: Let\u2019s think step by step. Pavel Urysohn is a mathematician. Leonid Levin is a mathematician and computer scientist. So Pavel Urysohn and Leonid Levin have the same type of work.\n Answer: Yes\n"], "prompt_template": "lambda input: f\"Question:\"", "task": "qa-open-domain"}}, "dataset_configs": null, "example": {"input": {"question": "What government position was held by the woman who portrayed Corliss Archer in the film Kiss and Tell ?"}, "output": "Chief of Protocol"}, "example_prompt": "Question: What is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?\nThought: Let\u2019s think step by step. The eastern sector of Colorado orogeny extends into the High Plains. High Plains rise in elevation from around 1,800 to 7,000 ft, so the answer is 1,800 to 7,000 ft. \nAnswer: 1,800 to 7,000 ft\n\nQuestion: Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who? Thought: Let\u2019s think step by step. Milhouse was named after U.S. president Richard Nixon, so the answer is Richard Nixon. \nAnswer: Richard Nixon\n\nQuestion: Which documentary is about Finnish rock groups, Adam Clayton Powell or The Saimaa Gesture? Thought: Let\u2019s think step by step. Adam Clayton Powell (film) is a documentary about an African-American politician, not Finnish rock groups. So the documentary about Finnish rock groups must instead be The Saimaa Gesture.\n Answer: The Saimaa Gesture\n\nQuestion: What profession does Nicholas Ray and Elia Kazan have in common?\n Thought: Let\u2019s think step by step. Professions of Nicholas Ray are director, screenwriter, and actor. Professions of Elia Kazan are director, producer, screenwriter, and actor. So profession Nicholas Ray and Elia Kazan have in common is director, screenwriter, and actor.\n Answer: director, screenwriter, actor\n\nQuestion: Which magazine was started first Arthur\u2019s Magazine or First for Women?\n Thought: Let\u2019s think step by step. Arthur\u2019s Magazine was started in 1844. First for Women was started in 1989. 1844 (Arthur\u2019s Magazine) < 1989 (First for Women), so Arthur\u2019s Magazine was started first.\n Answer: Arthur\u2019s Magazine\n\nQuestion: Were Pavel Urysohn and Leonid Levin known for the same type of work?\n Thought: Let\u2019s think step by step. Pavel Urysohn is a mathematician. Leonid Levin is a mathematician and computer scientist. So Pavel Urysohn and Leonid Levin have the same type of work.\n Answer: Yes\n\nQuestion: What government position was held by the woman who portrayed Corliss Archer in the film Kiss and Tell ?\n Thought: \n"}}
{"named_entity_recognition---default": {"description": "This promptware is used to extract entity in the text", "creator": "Promptware Authors", "homepage": "https://github.com/expressai/promptware", "reference": "", "codebase_url": "https://github.com/expressai/promptware/tree/main/softwares", "license": "apache-2.0", "task": null, "research_tasks": ["named-entity-recognition"], "application_categories": ["transformation"], "application_subcategories": ["extraction"], "original_platform": "gpt3", "design_pattern": "standalone", "source_language": "en", "target_language": "en", "module_name": "named_entity_recognition_promptware", "config_name": "default", "kernel_configs": {"openai": {"platform": "openai", "model_name": "text-curie-001", "max_tokens": 64, "temperature": 0, "top_p": 1, "suffix": "", "frequency_penalty": 0.0, "presence_penalty": 0.0, "logprobs": 1, "n": 1, "echo": false, "stop": null}}, "software_configs": {"sentiment_classification": {"name": "named-entity-recognition", "description": "This promptware is used to extract entities in the text", "instruction": "", "demonstration": ["I will go to Beijing on Saturday.\nEntity: Beijing.\n\nHe is a student in Peking University.\nEntity: Peking University\n\n"], "prompt_template": "lambda input: f\"{input['text']}\\nEntity:\",", "task": "named-entity-recognition"}}, "dataset_configs": null, "example": {"input": {"text": "I will go to New York on Saturday."}, "output": "New York."}, "example_prompt": "I will go to Beijing on Saturday.\nEntity: Beijing.\n\nHe is a student in Peking University.\nEntity: Peking University\n\n\nI will go to New York on Saturday.\nEntity:\n"}}
{"natural_language_inference---default": {"description": "This promptware is used to identify the semantic relationship between", "creator": "Promptware Authors", "homepage": "https://github.com/expressai/promptware", "reference": "", "codebase_url": "https://github.com/expressai/promptware/tree/main/softwares", "license": "apache-2.0", "task": null, "research_tasks": ["text-pair-classification"], "application_categories": ["classification"], "application_subcategories": ["general-classification"], "original_platform": "gpt3", "design_pattern": "standalone", "source_language": "en", "target_language": "en", "module_name": "natural_language_inference_promptware", "config_name": "default", "kernel_configs": {"openai": {"platform": "openai", "model_name": "text-curie-001", "max_tokens": 64, "temperature": 0, "top_p": 1, "suffix": "", "frequency_penalty": 0.0, "presence_penalty": 0.0, "logprobs": 1, "n": 1, "echo": false, "stop": null}}, "software_configs": {"sentiment_classification": {"name": "natural_language_inference", "description": "This promptware is used to identify the semantic relationship between two sentences", "instruction": "Give two sentences, predict which of the following categories their relationship falls into: entailment, contradiction, neutral", "demonstration": ["A person on a horse jumps over a broken down airplane.\tA person is training his horse for a competition.\nneutral", "A person on a horse jumps over a broken down airplane.\tA person is outdoors, on a horse.\nentailment", "Children smiling and waving at camera\tThe kids are frowning.\ncontradiction"], "prompt_template": "lambda input: f\"{input['text1']}\\t{input['text2']}\",", "task": "text-pair-classification"}}, "dataset_configs": null, "example": {"input": {"text1": "A boy is jumping on skateboard in the middle of a red bridge.", "text2": "The boy does a skateboarding trick."}, "output": "neutral"}, "example_prompt": "Give two sentences, predict which of the following categories their relationship falls into: entailment, contradiction, neutral\nA person on a horse jumps over a broken down airplane.\tA person is training his horse for a competition.\nneutral\nA person on a horse jumps over a broken down airplane.\tA person is outdoors, on a horse.\nentailment\nChildren smiling and waving at camera\tThe kids are frowning.\ncontradiction\nA boy is jumping on skateboard in the middle of a red bridge.\tThe boy does a skateboarding trick.\n"}}
{"natural_language_to_openai_api---default": {"description": "This promptware is used to convert natural lanugage to OpenAI API.", "creator": "OpenAI", "homepage": "https://beta.openai.com/examples/", "reference": "", "codebase_url": "https://beta.openai.com/examples/", "license": "no-license", "task": null, "research_tasks": ["conditional-generation"], "application_categories": ["transformation"], "application_subcategories": ["translation"], "original_platform": "gpt3", "design_pattern": "standalone", "source_language": "en", "target_language": "python", "module_name": "natural_language_to_open_aiapi_promptware", "config_name": "default", "kernel_configs": {"openai": {"platform": "openai", "model_name": "code-davinci-002", "max_tokens": 64, "temperature": 0, "top_p": 1.0, "suffix": "", "frequency_penalty": 0.0, "presence_penalty": 0.0, "logprobs": 1, "n": 1, "echo": false, "stop": ["\"\"\""]}}, "software_configs": {"natural_language_to_openai_api": {"name": "natural_lanugage_to_openai_api", "description": "This promptware is used to convert natural lanugage to OpenAI API.", "instruction": "\"\"\"\nUtil exposes the following:\nutil.openai() -> authenticates & returns the openai module, which has the following functions:\nopenai.Completion.create(\n    prompt=\"<my prompt>\", # The prompt to start completing from\n    max_tokens=123, # The max number of tokens to generate\n    temperature=1.0 # A measure of randomness\n    echo=True, # Whether to return the prompt in addition to the generated completion\n)\n", "demonstration": [], "prompt_template": "lambda input: f\"{input['text']}\",", "task": "conditional-generation"}}, "dataset_configs": null, "example": {"input": {"text": "\"\"\"\nimport util\n\"\"\"\nCreate an OpenAI completion starting from the prompt \"Once upon an AI\", no more than 5 tokens. Does not include the prompt.\n\"\"\"\n"}, "output": "def completion_example():\n    openai = util.openai()\n    completion = openai.Completion.create(\n        prompt=\"Once upon an AI\",\n        max_tokens=5,\n        temperature=1.0,\n        echo=False,\n    )"}, "example_prompt": "\"\"\"\nUtil exposes the following:\nutil.openai() -> authenticates & returns the openai module, which has the following functions:\nopenai.Completion.create(\n    prompt=\"<my prompt>\", # The prompt to start completing from\n    max_tokens=123, # The max number of tokens to generate\n    temperature=1.0 # A measure of randomness\n    echo=True, # Whether to return the prompt in addition to the generated completion\n)\n\n\n\"\"\"\nimport util\n\"\"\"\nCreate an OpenAI completion starting from the prompt \"Once upon an AI\", no more than 5 tokens. Does not include the prompt.\n\"\"\"\n\n"}}
{"natural_language_to_stripe_api---default": {"description": "This promptware is used to convert natural lanugage to Stripe API.", "creator": "OpenAI", "homepage": "https://beta.openai.com/examples/", "reference": "", "codebase_url": "https://beta.openai.com/examples/", "license": "no-license", "task": null, "research_tasks": ["conditional-generation"], "application_categories": ["transformation"], "application_subcategories": ["translation"], "original_platform": "gpt3", "design_pattern": "standalone", "source_language": "en", "target_language": "javascript", "module_name": "natural_language_to_stripe_api_promptware", "config_name": "default", "kernel_configs": {"openai": {"platform": "openai", "model_name": "code-davinci-002", "max_tokens": 100, "temperature": 0, "top_p": 1.0, "suffix": "", "frequency_penalty": 0.0, "presence_penalty": 0.0, "logprobs": 1, "n": 1, "echo": false, "stop": ["\"\"\""]}}, "software_configs": {"natural_language_to_stripe_api": {"name": "natural_lanugage_to_stripe_api", "description": "This promptware is used to convert natural lanugage to Stripe API.", "instruction": "\"\"\"\nUtil exposes the following:\n\nutil.stripe() -> authenticates & returns the stripe module; usable as stripe.Charge.create etc\n\"\"\"\n", "demonstration": [], "prompt_template": "lambda input: f\"{input['text']}\",", "task": "conditional-generation"}}, "dataset_configs": null, "example": {"input": {"text": "import util\n\"\"\"\nCreate a Stripe token using the users credit card: 5555-4444-3333-2222, expiration date 12 / 28, cvc 521\n\"\"\""}, "output": "token = stripe.Token.create(\n    card={\n        \"number\": \"5555-4444-3333-2222\",\n        \"exp_month\": 12,\n        \"exp_year\": 28,\n        \"cvc\": \"521\"\n    },\n)"}, "example_prompt": "\"\"\"\nUtil exposes the following:\n\nutil.stripe() -> authenticates & returns the stripe module; usable as stripe.Charge.create etc\n\"\"\"\n\n\nimport util\n\"\"\"\nCreate a Stripe token using the users credit card: 5555-4444-3333-2222, expiration date 12 / 28, cvc 521\n\"\"\"\n"}}
{"parse_unstructured_data---default": {"description": "Answer questions based on existing knowledge.", "creator": "OpenAI", "homepage": "https://beta.openai.com/examples/default-parse-data", "reference": "", "codebase_url": "https://beta.openai.com/examples/default-parse-data", "license": "no-license", "task": null, "research_tasks": ["conditional-generation"], "application_categories": ["transformation"], "application_subcategories": ["extraction"], "original_platform": "gpt3", "design_pattern": "standalone", "source_language": "en", "target_language": "en", "module_name": "parse_unstructured_data_promptware", "config_name": "default", "kernel_configs": {"openai": {"platform": "openai", "model_name": "text-curie-001", "max_tokens": 100, "temperature": 0, "top_p": 1, "suffix": "", "frequency_penalty": 0.0, "presence_penalty": 0.0, "logprobs": 1, "n": 1, "echo": false, "stop": null}}, "software_configs": {"parse_unstructured_data": {"name": "parse_unstructured_data", "description": "Create tables from long form text by specifying a structure and supplying some examples.", "instruction": "A table summarizing the following text.", "demonstration": [], "prompt_template": "lambda input: f\"{input['text']}\\n{input['head']}\\n\",", "task": "conditional-generation"}}, "dataset_configs": null, "example": {"input": {"text": "There are many fruits that were found on the recently discovered planet Goocrux. There are neoskizzles that grow there, which are purple and taste like candy. There are also loheckles, which are a grayish blue fruit and are very tart, a little bit like a lemon. Pounits are a bright green color and are more savory than sweet. There are also plenty of loopnovas which are a neon pink flavor and taste like cotton candy. Finally, there are fruits called glowls, which have a very sour and bitter taste which is acidic and caustic, and a pale orange tinge to them.", "head": "| Fruit | Color | Flavor |"}, "output": "There are many fruits that were found on the recently discovered planet Goocrux. There are neoskizzles that grow there, which are purple and taste like candy. There are also loheckles, which are a grayish blue fruit and are very tart, a little bit like a lemon. Pounits are a bright green color and are more savory than sweet. There are also plenty of loopnovas which are a neon pink flavor and taste like cotton candy"}, "example_prompt": "A table summarizing the following text.\n\nThere are many fruits that were found on the recently discovered planet Goocrux. There are neoskizzles that grow there, which are purple and taste like candy. There are also loheckles, which are a grayish blue fruit and are very tart, a little bit like a lemon. Pounits are a bright green color and are more savory than sweet. There are also plenty of loopnovas which are a neon pink flavor and taste like cotton candy. Finally, there are fruits called glowls, which have a very sour and bitter taste which is acidic and caustic, and a pale orange tinge to them.\n| Fruit | Color | Flavor |\n\n"}}
{"product_name_generator---default": {"description": "This promptware is used to create product names from examples words.", "creator": "OpenAI", "homepage": "https://beta.openai.com/examples/", "reference": "", "codebase_url": "https://beta.openai.com/examples/", "license": "no-license", "task": null, "research_tasks": ["conditional-generation"], "application_categories": ["generation"], "application_subcategories": ["text-generation"], "original_platform": "gpt3", "design_pattern": "standalone", "source_language": "en", "target_language": "en", "module_name": "product_name_generator_promptware", "config_name": "default", "kernel_configs": {"openai": {"platform": "openai", "model_name": "text-davinci-003", "max_tokens": 60, "temperature": 0.8, "top_p": 1.0, "suffix": "", "frequency_penalty": 0.0, "presence_penalty": 0.0, "logprobs": 1, "n": 1, "echo": false, "stop": null}}, "software_configs": {"product_name_generator": {"name": "product_name_generator", "description": "This promptware is used to create product names from examples words.", "instruction": "", "demonstration": ["Product description: A home milkshake maker\nSeed words: fast, healthy, compact.\nProduct names: HomeShaker, Fit Shaker, QuickShake, Shake Maker\n\n"], "prompt_template": "lambda input: f\"{input['text']}\",", "task": "conditional-generation"}}, "dataset_configs": null, "example": {"input": {"text": "Product description: A pair of shoes that can fit any foot size.\nSeed words: adaptable, fit, omni-fit."}, "output": "Product names: AdaptFits, OmniShoes, FitAlls, FootFits."}, "example_prompt": "Product description: A home milkshake maker\nSeed words: fast, healthy, compact.\nProduct names: HomeShaker, Fit Shaker, QuickShake, Shake Maker\n\n\nProduct description: A pair of shoes that can fit any foot size.\nSeed words: adaptable, fit, omni-fit.\n"}}
{"prompt_engineer---default": {"description": "GPT3 is a good prompt engineer and it can help us to generate an instruction based on input and output pairs.", "creator": "Promptware Authors", "homepage": "https://github.com/expressai/promptware", "reference": "", "codebase_url": "https://github.com/expressai/promptware/tree/main/softwares", "license": "apache-2.0", "task": null, "research_tasks": ["conditional-generation"], "application_categories": ["generation"], "application_subcategories": ["text-generation"], "original_platform": "gpt3", "design_pattern": "standalone", "source_language": "en", "target_language": "en", "module_name": "prompt_engineer_promptware", "config_name": "default", "kernel_configs": {"openai": {"platform": "openai", "model_name": "text-davinci-002", "max_tokens": 256, "temperature": 0, "top_p": 1, "suffix": "", "frequency_penalty": 0.0, "presence_penalty": 0.0, "logprobs": 1, "n": 1, "echo": false, "stop": null}}, "software_configs": {"prompt_engineer_complete": {"name": "prompt_engineer_complete", "description": "This promptware is used to generate an instruction based on given input pairs", "instruction": "GPT3 is a good prompt engineer and it can help us to generate instruction based on input and output pairs.\n", "demonstration": ["I gave a friend an instruction. Based on the instruction they produced the following input-output pairs:\n\nInput: I love this movie.\nOutput: positive\n\nInput: The movie is boring.\nOutput: negative\n\nThe instruction was to  find: the sentiment of the movie review.\n"], "prompt_template": "lambda input: f\"I gave a friend an instruction.\"", "task": "conditional-generation"}}, "dataset_configs": null, "example": {"input": [["aperiodic", "periodic"], ["unsent", "sent"]], "output": {"text": "The sentiment of the movie review.", "score": -0.25450913903571426}}, "example_prompt": "GPT3 is a good prompt engineer and it can help us to generate instruction based on input and output pairs.\n\nI gave a friend an instruction. Based on the instruction they produced the following input-output pairs:\n\nInput: I love this movie.\nOutput: positive\n\nInput: The movie is boring.\nOutput: negative\n\nThe instruction was to  find: the sentiment of the movie review.\n\nI gave a friend an instruction. Based on the instruction they produced the following input-output pairs:\n\nInput:aperiodic\nOutput:periodic\n\nInput:unsent\nOutput:sent\n\nThe instruction was to find:\n"}}
{"python_bug_fixer---default": {"description": "This promptware is used to structure the prompt for checking for bugs. Here we add a comment suggesting that source code is buggy, and then ask codex to generate a fixed code.", "creator": "OpenAI", "homepage": "https://beta.openai.com/examples/", "reference": "", "codebase_url": "https://beta.openai.com/examples/", "license": "no-license", "task": null, "research_tasks": ["conditional-generation"], "application_categories": ["transformation"], "application_subcategories": ["rewriting"], "original_platform": "gpt3", "design_pattern": "standalone", "source_language": "python", "target_language": "python", "module_name": "python_bug_fixer_promptware", "config_name": "default", "kernel_configs": {"openai": {"platform": "openai", "model_name": "text-davinci-003", "max_tokens": 182, "temperature": 0, "top_p": 1.0, "suffix": "", "frequency_penalty": 0.0, "presence_penalty": 0.0, "logprobs": 1, "n": 1, "echo": false, "stop": ["###"]}}, "software_configs": {"python_bug_fixer": {"name": "python_bug_fixer", "description": "This promptware is used to structure the prompt for checking for bugs. Here we add a comment suggesting that source code is buggy, and then ask codex to generate a fixed code.", "instruction": "##### Fix bugs in the below function\n \n", "demonstration": [], "prompt_template": "lambda input: f\"{input['text']}\",", "task": "conditional-generation"}}, "dataset_configs": null, "example": {"input": {"text": "### Buggy Python\nimport Random\na = random.randint(1,12)\nb = random.randint(1,12)\nfor i in range(10):\n    question = \"What is \"+a+\" x \"+b+\"? \"\n    answer = input(question)\n    if answer = a*b\n        print (Well done!)\n    else:\n        print(\"No.\")\n    \n### Fixed Python"}, "output": "import random\na = random.randint(1,12)\nb = random.randint(1,12)\nfor i in range(10):\n    question = \"What is \"+str(a)+\" x \"+str(b)+\"? \"\n    answer = int(input(question))\n    if answer == a*b:\n        print (\"Well done!\")\n    else:\n        print(\"No.\")"}, "example_prompt": "##### Fix bugs in the below function\n \n\n\n### Buggy Python\nimport Random\na = random.randint(1,12)\nb = random.randint(1,12)\nfor i in range(10):\n    question = \"What is \"+a+\" x \"+b+\"? \"\n    answer = input(question)\n    if answer = a*b\n        print (Well done!)\n    else:\n        print(\"No.\")\n    \n### Fixed Python\n"}}
{"query_doc_matching---default": {"description": "This software is to identify if a query semantically relevant to a document.", "creator": "Promptware Authors", "homepage": "https://github.com/expressai/promptware", "reference": "", "codebase_url": "https://github.com/expressai/promptware/tree/main/softwares", "license": "apache-2.0", "task": null, "research_tasks": ["text-pair-classification"], "application_categories": ["classification"], "application_subcategories": ["general-classification"], "original_platform": "gpt3", "design_pattern": "standalone", "source_language": "en", "target_language": "en", "module_name": "query_doc_matching_promptware", "config_name": "default", "kernel_configs": {"openai": {"platform": "openai", "model_name": "text-davinci-002", "max_tokens": 64, "temperature": 0, "top_p": 1, "suffix": "", "frequency_penalty": 0.0, "presence_penalty": 0.0, "logprobs": 1, "n": 1, "echo": false, "stop": null}}, "software_configs": {"query_doc_matching": {"name": "query_doc_matching", "description": "This software is to identify if a query semantically relevant to a document.", "instruction": "Decide whether the documents are relevant to the given search query:\n", "demonstration": ["Query: Jackie Chan's birthday\nDocuments: https://en.wikipedia.org/wiki/Jackie_Chan|Jackie Chan - Wikipedia|Fang Shilong (born 7 April 1954), known professionally in English as Jackie Chan and in Chinese as Cheng Long, is a Hong Kong actor, filmmaker, martial artist, and stuntman known for his slapstick acrobatic fighting style, comic timing, and innovative stunts, which he typically performs himself. Chan has been acting since the 1960s, performing in more than 150 films. He is one of the most popular action film stars of all time.\nResult: Relevant\n", "Query:Pengfei Liu in NLP\nDocuments:https://scholar.google.com/citations?user=oIz_CYEAAAAJ&hl=en|\u202aPengfei Liu\u202c - \u202aGoogle Scholar|Carnegie Mellon University -\u202aNatural Language Processing\u202c - \u202aText Summarization\u202c - \u202aDiagnosis for NLP Models\u202c - \u202aMulti-task/Transfer\nResult: Relevant\n\u202c", "Query: Chinese New Year 2023\nDocuments: https://en.wikipedia.org/wiki/Tang_dynasty|Tang dynasty - Wikipedia| The Tang dynasty, or Tang Empire, was an imperial dynasty of China that ruled from 618 to 907 AD, with an interregnum between 690 and 705. It was preceded by the Sui dynasty and followed by the Five Dynasties and Ten Kingdoms period.\nResult:Not relevant\n"], "prompt_template": "lambda input: f\"Query: {input['query']}\\nDocuments:\"", "task": "text-pair-classification"}}, "dataset_configs": null, "example": {"input": {"query": "Zhiyuan Liu address", "document": {"title": "Zhiyuan Liu", "link": "https://nlp.csai.tsinghua.edu.cn/~lzy/", "document": " I'm an associate professor at Tsinghua University. I am always looking for highly-motivated post-docs and visiting scholars to work together on natural language processing, knowledge graphs, and social computing. Please read the Natural Language Processing. Language: English. The degree course of CS graduate students (specially for the Master Program of Advanced Programming) at Tsinghua University. [ Foundations of Object-Oriented Programming. Language: Chinese. The compulsory course of CS undergraduate students at Tsinghua University."}}, "output": ""}, "example_prompt": "Decide whether the documents are relevant to the given search query:\n\nQuery: Jackie Chan's birthday\nDocuments: https://en.wikipedia.org/wiki/Jackie_Chan|Jackie Chan - Wikipedia|Fang Shilong (born 7 April 1954), known professionally in English as Jackie Chan and in Chinese as Cheng Long, is a Hong Kong actor, filmmaker, martial artist, and stuntman known for his slapstick acrobatic fighting style, comic timing, and innovative stunts, which he typically performs himself. Chan has been acting since the 1960s, performing in more than 150 films. He is one of the most popular action film stars of all time.\nResult: Relevant\n\nQuery:Pengfei Liu in NLP\nDocuments:https://scholar.google.com/citations?user=oIz_CYEAAAAJ&hl=en|\u202aPengfei Liu\u202c - \u202aGoogle Scholar|Carnegie Mellon University -\u202aNatural Language Processing\u202c - \u202aText Summarization\u202c - \u202aDiagnosis for NLP Models\u202c - \u202aMulti-task/Transfer\nResult: Relevant\n\u202c\nQuery: Chinese New Year 2023\nDocuments: https://en.wikipedia.org/wiki/Tang_dynasty|Tang dynasty - Wikipedia| The Tang dynasty, or Tang Empire, was an imperial dynasty of China that ruled from 618 to 907 AD, with an interregnum between 690 and 705. It was preceded by the Sui dynasty and followed by the Five Dynasties and Ten Kingdoms period.\nResult:Not relevant\n\nQuery: Zhiyuan Liu address\nDocuments:https://nlp.csai.tsinghua.edu.cn/~lzy/ | Zhiyuan Liu |  I'm an associate professor at Tsinghua University. I am always looking for highly-motivated post-docs and visiting scholars to work together on natural language processing, knowledge graphs, and social computing. Please read the Natural Language Processing. Language: English. The degree course of CS graduate students (specially for the Master Program of Advanced Programming) at Tsinghua University. [ Foundations of Object-Oriented Programming. Language: Chinese. The compulsory course of CS undergraduate students at Tsinghua University.\nResult: \n"}}
{"question_answer---default": {"description": "Answer questions based on existing knowledge.", "creator": "OpenAI", "homepage": "https://beta.openai.com/examples/", "reference": "", "codebase_url": "https://beta.openai.com/examples/", "license": "no-license", "task": null, "research_tasks": ["qa-open-domain"], "application_categories": ["conversation"], "application_subcategories": ["question-answering"], "original_platform": "gpt3", "design_pattern": "standalone", "source_language": "en", "target_language": "en", "module_name": "question_answer_promptware", "config_name": "default", "kernel_configs": {"openai1": {"platform": "openai", "model_name": "text-curie-001", "max_tokens": 100, "temperature": 0, "top_p": 1, "suffix": "", "frequency_penalty": 0.0, "presence_penalty": 0.0, "logprobs": 1, "n": 1, "echo": false, "stop": null}, "openai2": {"platform": "openai", "model_name": "text-curie-001", "max_tokens": 60, "temperature": 0.3, "top_p": 1, "suffix": "", "frequency_penalty": 0.5, "presence_penalty": 0, "logprobs": 1, "n": 1, "echo": false, "stop": null}, "openai3": {"platform": "openai", "model_name": "text-curie-001", "max_tokens": 60, "temperature": 0, "top_p": 1, "suffix": "", "frequency_penalty": 0.5, "presence_penalty": 0.0, "logprobs": 1, "n": 1, "echo": false, "stop": null}, "openai4": {"platform": "openai", "model_name": "text-curie-001", "max_tokens": 60, "temperature": 0, "top_p": 1, "suffix": "", "frequency_penalty": 0.0, "presence_penalty": 0.0, "logprobs": 1, "n": 1, "echo": false, "stop": null}}, "software_configs": {"question_answer_general": {"name": "question_answer_general", "description": "Answer questions based on existing knowledge.", "instruction": "I am a highly intelligent question answering bot. If you ask me a question that is rooted in truth, I will give you the answer. If you ask me a question that is nonsense, trickery, or has no clear answer, I will respond with \"Unknown\".", "demonstration": ["Q: What is human life expectancy in the United States?\nA: Human life expectancy in the United States is 78 years.", "Q: Who was president of the United States in 1955?\nA: Dwight D. Eisenhower was president of the United States in 1955.", "Q: Which party did he belong to?\nA: He belonged to the Republican Party.", "Q: What is the square root of banana?\nA: Unknown", "Q: How does a telescope work?\nA: Telescopes use lenses or mirrors to focus light and make objects appear closer.", "Q: Where were the 1992 Olympics held?\nA: The 1992 Olympics were held in Barcelona, Spain."], "prompt_template": "lambda input: f\"Q: {input['question']}\\nA:\",", "task": "qa-open-domain"}}, "dataset_configs": null, "example": {"input": {"question": "Where is the Valley of Kings?"}, "output": "The Valley of Kings is located in Egypt."}, "example_prompt": "I am a highly intelligent question answering bot. If you ask me a question that is rooted in truth, I will give you the answer. If you ask me a question that is nonsense, trickery, or has no clear answer, I will respond with \"Unknown\".\nQ: What is human life expectancy in the United States?\nA: Human life expectancy in the United States is 78 years.\nQ: Who was president of the United States in 1955?\nA: Dwight D. Eisenhower was president of the United States in 1955.\nQ: Which party did he belong to?\nA: He belonged to the Republican Party.\nQ: What is the square root of banana?\nA: Unknown\nQ: How does a telescope work?\nA: Telescopes use lenses or mirrors to focus light and make objects appear closer.\nQ: Where were the 1992 Olympics held?\nA: The 1992 Olympics were held in Barcelona, Spain.\nQ: Where is the Valley of Kings?\nA:\n"}}
{"react---default": {"description": "This promptware is used to answer a question that involves multiple step reasoning with chain-of-thought method.", "creator": "Promptware Authors", "homepage": "https://github.com/expressai/promptware", "reference": "", "codebase_url": "https://github.com/expressai/promptware/tree/main/softwares", "license": "apache-2.0", "task": null, "research_tasks": ["qa-open-domain"], "application_categories": ["conversation"], "application_subcategories": ["question-answering"], "original_platform": "gpt3", "design_pattern": "standalone", "source_language": "en", "target_language": "en", "module_name": "react_promptware", "config_name": "default", "kernel_configs": {"openai": {"platform": "openai", "model_name": "text-davinci-003", "max_tokens": 64, "temperature": 0, "top_p": 1, "suffix": "", "frequency_penalty": 0.0, "presence_penalty": 0.0, "logprobs": 1, "n": 1, "echo": false, "stop": null}}, "software_configs": {"multi_hop_qa": {"name": "multi_hop_qa", "description": "This promptware is used to answer a question that involves multiple step reasoning with react method.", "instruction": "", "demonstration": ["Question: What is the elevation range for the area that the eastern sector of the Colorado orogeny extends into? \nThought 1: I need to search Colorado orogeny, find the area that the eastern sector of the Colorado orogeny extends into, then find the elevation range of the area.\nAction 1: Search[Colorado orogeny]\nObservation 1: The Colorado orogeny was an episode of mountain building (an orogeny) in Colorado and surrounding areas.\nThought 2: It does not mention the eastern sector. So I need to look up eastern sector.Action 2: Lookup[eastern sector]Observation 2: (Result 1 / 1) The eastern sector extends into the High Plains and is called the Central Plains orogeny.\n"], "prompt_template": "lambda input: f\"Question: {input['question']}\",", "task": "qa-open-domain"}}, "dataset_configs": null, "example": {"input": {"question": "What government position was held by the woman who portrayed Corliss Archer in the film Kiss and Tell ?"}, "output": "Chief of Protocol"}, "example_prompt": "Question: What is the elevation range for the area that the eastern sector of the Colorado orogeny extends into? \nThought 1: I need to search Colorado orogeny, find the area that the eastern sector of the Colorado orogeny extends into, then find the elevation range of the area.\nAction 1: Search[Colorado orogeny]\nObservation 1: The Colorado orogeny was an episode of mountain building (an orogeny) in Colorado and surrounding areas.\nThought 2: It does not mention the eastern sector. So I need to look up eastern sector.Action 2: Lookup[eastern sector]Observation 2: (Result 1 / 1) The eastern sector extends into the High Plains and is called the Central Plains orogeny.\n\nQuestion: What government position was held by the woman who portrayed Corliss Archer in the film Kiss and Tell ?\n"}}
{"recipe_creator---default": {"description": "This promptware is used to create a recipe from a list of ingredients.", "creator": "OpenAI", "homepage": "https://beta.openai.com/examples/", "reference": "", "codebase_url": "https://beta.openai.com/examples/", "license": "no-license", "task": null, "research_tasks": ["conditional-generation"], "application_categories": ["brainstorming"], "application_subcategories": ["how-to-generation"], "original_platform": "gpt3", "design_pattern": "standalone", "source_language": "en", "target_language": "en", "module_name": "recipe_creator_promptware", "config_name": "default", "kernel_configs": {"openai": {"platform": "openai", "model_name": "text-davinci-003", "max_tokens": 120, "temperature": 0.3, "top_p": 1.0, "suffix": "", "frequency_penalty": 0.0, "presence_penalty": 0.0, "logprobs": 1, "n": 1, "echo": false, "stop": null}}, "software_configs": {"recipe_creator": {"name": "recipe_creator", "description": "This promptware is used to create a recipe from a list of ingredients.", "instruction": "Write a recipe based on these ingredients and instructions:\n\n", "demonstration": [], "prompt_template": "lambda input: f\"{input['text']}\",", "task": "conditional-generation"}}, "dataset_configs": null, "example": {"input": {"text": "Frito Pie\n\nIngredients:\nFritos\nChili\nShredded cheddar cheese\nSweet white or red onions, diced small\nSour cream\n\nInstructions:"}, "output": "1. Preheat oven to 350 degrees F.\n\n2. Spread a layer of Fritos in the bottom of a 9x13 inch baking dish.\n\n3. Top with chili and spread evenly.\n\n4. Sprinkle shredded cheese over chili.\n\n5. Add diced onions over cheese.\n\n6. Bake in preheated oven for 20 minutes.\n\n7. Remove from oven and top with a dollop of sour cream.\n\n8. Serve and enjoy!"}, "example_prompt": "Write a recipe based on these ingredients and instructions:\n\n\n\nFrito Pie\n\nIngredients:\nFritos\nChili\nShredded cheddar cheese\nSweet white or red onions, diced small\nSour cream\n\nInstructions:\n"}}
{"restaurant_review_creator---default": {"description": "This promptware is used to turn a few words into a restaurant review.", "creator": "OpenAI", "homepage": "https://beta.openai.com/examples/", "reference": "", "codebase_url": "https://beta.openai.com/examples/", "license": "no-license", "task": null, "research_tasks": ["conditional-generation"], "application_categories": ["generation"], "application_subcategories": ["text-generation"], "original_platform": "gpt3", "design_pattern": "standalone", "source_language": "en", "target_language": "en", "module_name": "restaurant_review_creator_promptware", "config_name": "default", "kernel_configs": {"openai": {"platform": "openai", "model_name": "text-davinci-003", "max_tokens": 64, "temperature": 0.5, "top_p": 1.0, "suffix": "", "frequency_penalty": 0.0, "presence_penalty": 0.0, "logprobs": 1, "n": 1, "echo": false, "stop": null}}, "software_configs": {"restaurant_review_creator": {"name": "restaurant_review_creator", "description": "This promptware is used to turn a few words into a restaurant review.", "instruction": "Write a restaurant review based on these notes:\n\n", "demonstration": [], "prompt_template": "lambda input: f\"{input['text']}\",", "task": "conditional-generation"}}, "dataset_configs": null, "example": {"input": {"text": "Name: The Blue Wharf\nLobster great, noisy, service polite, prices good.\n\nReview:"}, "output": "I recently visited The Blue Wharf for dinner and had a great experience. The lobster was especially delicious - definitely the highlight of the meal. The only downside was that it was a bit noisy, but the polite service more than made up for it. Prices were also very reasonable, making it a great value overall."}, "example_prompt": "Write a restaurant review based on these notes:\n\n\n\nName: The Blue Wharf\nLobster great, noisy, service polite, prices good.\n\nReview:\n"}}
{"science_fiction_book_list_maker---default": {"description": "This promptware is used to make a list of science fiction books and stop when it reaches #10.", "creator": "OpenAI", "homepage": "https://beta.openai.com/examples/", "reference": "", "codebase_url": "https://beta.openai.com/examples/", "license": "no-license", "task": null, "research_tasks": ["conditional-generation"], "application_categories": ["brainstorming"], "application_subcategories": ["recommendation"], "original_platform": "gpt3", "design_pattern": "standalone", "source_language": "en", "target_language": "en", "module_name": "science_fiction_book_list_maker_promptware", "config_name": "default", "kernel_configs": {"openai": {"platform": "openai", "model_name": "text-davinci-003", "max_tokens": 200, "temperature": 0.5, "top_p": 1.0, "suffix": "", "frequency_penalty": 0.52, "presence_penalty": 0.5, "logprobs": 1, "n": 1, "echo": false, "stop": ["11."]}}, "software_configs": {"science_fiction_book_list_maker": {"name": "science_fiction_book_list_maker", "description": "This promptware is used to make a list of science fiction books and stop when it reaches #10.", "instruction": "", "demonstration": [], "prompt_template": "lambda input: f\"{input['text']}\",", "task": "conditional-generation"}}, "dataset_configs": null, "example": {"input": {"text": "List 10 science fiction books:"}, "output": "1. 1984 by George Orwell\n2. Dune by Frank Herbert\n3. The Hitchhiker's Guide to the Galaxy by Douglas Adams\n4. The War of the Worlds by H.G. Wells\n5. Brave New World by Aldous Huxley\n6. Ender's Game by Orson Scott Card\n7. The Martian Chronicles by Ray Bradbury\n8. Do Androids Dream of Electric Sheep? By Philip K Dick \n9. Snow Crash by Neal Stephenson \n10. Neuromancer by William Gibson"}, "example_prompt": "\nList 10 science fiction books:\n"}}
{"scoring---default": {"description": "This software is used to calculate the average log probability of a text", "creator": "Promptware Authors", "homepage": "https://github.com/expressai/promptware", "reference": "", "codebase_url": "https://github.com/expressai/promptware/tree/main/softwares", "license": "apache-2.0", "task": null, "research_tasks": ["others"], "application_categories": ["others"], "application_subcategories": ["others"], "original_platform": "gpt3", "design_pattern": "standalone", "source_language": "en", "target_language": "en", "module_name": "scoring_promptware", "config_name": "default", "kernel_configs": {"openai": {"platform": "openai", "model_name": "text-curie-001", "max_tokens": 0, "temperature": 0, "top_p": 1, "suffix": "", "frequency_penalty": 0, "presence_penalty": 0, "logprobs": 1, "n": 1, "echo": true, "stop": null}}, "software_configs": {"scoring": {"name": "scoring", "description": "Given a text, generate the average of its log probabilities", "instruction": "", "demonstration": null, "prompt_template": "lambda text: f\"{text}\",", "task": "others"}}, "dataset_configs": null, "example": {"input": "I love this movie", "output": -3.8613948799999998}, "example_prompt": "I love this movie\n"}}
{"sentiment_classifier---default": {"description": "This promptware is used to identify the sentiment of a sentence (positive or negative) based on some learning", "creator": "Promptware Authors", "homepage": "https://github.com/expressai/promptware", "reference": "", "codebase_url": "https://github.com/expressai/promptware/tree/main/softwares", "license": "apache-2.0", "task": null, "research_tasks": ["text-classification"], "application_categories": ["classification"], "application_subcategories": ["sentiment-analysis"], "original_platform": "gpt3", "design_pattern": "standalone", "source_language": "en", "target_language": "en", "module_name": "sentiment_classifier_promptware", "config_name": "default", "kernel_configs": {"openai": {"platform": "openai", "model_name": "text-curie-001", "max_tokens": 64, "temperature": 0, "top_p": 1, "suffix": "", "frequency_penalty": 0.0, "presence_penalty": 0.0, "logprobs": 1, "n": 1, "echo": false, "stop": null}}, "software_configs": {"sentiment_classification": {"name": "sentiment_classification", "description": "This promptware is used to identify the sentiment of a sentence (positive or negative) based on some learning samples from the sst2 dataset.", "instruction": "Give a sentence, classify the sentiment of it using negative and positive labels", "demonstration": ["I love this movie.\npositive", "This movie is too boring.\nnegative"], "prompt_template": "lambda input: f\"{input['text']}\",", "task": "text-classification"}}, "dataset_configs": {"sst2": {"dataset_name": "sst2", "sub_dataset": null, "split": "test", "n_samples": 10}}, "example": {"input": {"text": "I love this movie."}, "output": "positive"}, "example_prompt": "Give a sentence, classify the sentiment of it using negative and positive labels\nI love this movie.\npositive\nThis movie is too boring.\nnegative\nI love this movie.\n"}}
{"spreadsheet_creator---default": {"description": "This promptware is used to create spreadsheets of various kinds of data. It's a long prompt but very versatile. Output can be copy+pasted into a text file and saved as a .csv with pipe separators.", "creator": "OpenAI", "homepage": "https://beta.openai.com/examples/", "reference": "", "codebase_url": "https://beta.openai.com/examples/", "license": "no-license", "task": null, "research_tasks": ["conditional-generation"], "application_categories": ["generation"], "application_subcategories": ["data-generation"], "original_platform": "gpt3", "design_pattern": "standalone", "source_language": "en", "target_language": "en", "module_name": "spreadsheet_creator_promptware", "config_name": "default", "kernel_configs": {"openai": {"platform": "openai", "model_name": "text-davinci-003", "max_tokens": 60, "temperature": 0.5, "top_p": 1.0, "suffix": "", "frequency_penalty": 0.0, "presence_penalty": 0.0, "logprobs": 1, "n": 1, "echo": false, "stop": null}}, "software_configs": {"spreadsheet_creator": {"name": "spreadsheet_creator", "description": "This promptware is used to create spreadsheets of various kinds of data. It's a long prompt but very versatile. Output can be copy+pasted into a text file and saved as a .csv with pipe separators.", "instruction": "", "demonstration": [], "prompt_template": "lambda input: f\"{input['text']}\",", "task": "conditional-generation"}}, "dataset_configs": null, "example": {"input": {"text": "A two-column spreadsheet of top science fiction movies and the year of release:\n\nTitle |  Year of release"}, "output": "----------------------------\nBlade Runner  | 1982\nThe Matrix | 1999\nAlien | 1979\nThe Terminator | 1984\nBack to the Future | 1985\nStar Wars | 1977\nE.T. the Extra-Terrestrial | 1982\n2001: A Space Odyssey | 1968\nThe Day the Earth St"}, "example_prompt": "\nA two-column spreadsheet of top science fiction movies and the year of release:\n\nTitle |  Year of release\n"}}
{"sql_request---default": {"description": "This promptware is used to create simple SQL queries.", "creator": "OpenAI", "homepage": "https://beta.openai.com/examples/", "reference": "", "codebase_url": "https://beta.openai.com/examples/", "license": "no-license", "task": null, "research_tasks": ["conditional-generation"], "application_categories": ["generation"], "application_subcategories": ["code-generation"], "original_platform": "gpt3", "design_pattern": "standalone", "source_language": "en", "target_language": "sql", "module_name": "sql_request_promptware", "config_name": "default", "kernel_configs": {"openai": {"platform": "openai", "model_name": "text-davinci-003", "max_tokens": 60, "temperature": 0.3, "top_p": 1.0, "suffix": "", "frequency_penalty": 0.0, "presence_penalty": 0.0, "logprobs": 1, "n": 1, "echo": false, "stop": null}}, "software_configs": {"sql_request": {"name": "sql_request", "description": "This promptware is used to create simple SQL queries.", "instruction": "", "demonstration": [], "prompt_template": "lambda input: f\"{input['text']}\",", "task": "conditional-generation"}}, "dataset_configs": null, "example": {"input": {"text": "Create a SQL request to find all users who live in California and have over 1000 credits:"}, "output": "SELECT * FROM users\nWHERE state = 'California'\nAND credits > 1000;"}, "example_prompt": "\nCreate a SQL request to find all users who live in California and have over 1000 credits:\n"}}
{"sql_translate---default": {"description": "This promptware is used to translate natural language to SQL queries.", "creator": "OpenAI", "homepage": "https://beta.openai.com/examples/", "reference": "", "codebase_url": "https://beta.openai.com/examples/", "license": "no-license", "task": null, "research_tasks": ["conditional-generation"], "application_categories": ["transformation"], "application_subcategories": ["translation"], "original_platform": "gpt3", "design_pattern": "standalone", "source_language": "en", "target_language": "sql", "module_name": "sql_translate_promptware", "config_name": "default", "kernel_configs": {"openai": {"platform": "openai", "model_name": "code-davinci-002", "max_tokens": 150, "temperature": 0, "top_p": 1.0, "suffix": "", "frequency_penalty": 0.0, "presence_penalty": 0.0, "logprobs": 1, "n": 1, "echo": false, "stop": ["#", ";"]}}, "software_configs": {"sql_translate": {"name": "sql_translate", "description": "This promptware is used to translate natural language to SQL queries.", "instruction": "### Postgres SQL tables, with their properties:\n#\n# Employee(id, name, department_id)\n# Department(id, name, address)\n# Salary_Payments(id, employee_id, amount, date)\n#\n### ", "demonstration": [], "prompt_template": "lambda input: f\"{input['text']}\",", "task": "conditional-generation"}}, "dataset_configs": null, "example": {"input": {"text": "A query to list the names of the departments which employed more than 10 employees in the last 3 months\nSELECT"}, "output": "d.name\nFROM\n    Department d\n    INNER JOIN Employee e ON d.id = e.department_id\n    INNER JOIN Salary_Payments sp ON e.id = sp.employee_id\nWHERE\n    sp.date >= DATE_SUB(CURDATE(), INTERVAL 3 MONTH)\nGROUP BY\n    d.name\nHAVING\n    COUNT(e.id) > 10"}, "example_prompt": "### Postgres SQL tables, with their properties:\n#\n# Employee(id, name, department_id)\n# Department(id, name, address)\n# Salary_Payments(id, employee_id, amount, date)\n#\n### \n\nA query to list the names of the departments which employed more than 10 employees in the last 3 months\nSELECT\n"}}
{"summarization---default": {"description": "Summarize long text to short text.", "creator": "OpenAI", "homepage": "https://beta.openai.com/examples/", "reference": "", "codebase_url": "https://beta.openai.com/examples/", "license": "no-license", "task": null, "research_tasks": ["summarization"], "application_categories": ["transformation"], "application_subcategories": ["summarization"], "original_platform": "gpt3", "design_pattern": "standalone", "source_language": "en", "target_language": "en", "module_name": "summarization_promptware", "config_name": "default", "kernel_configs": {"openai1": {"platform": "openai", "model_name": "text-curie-001", "max_tokens": 200, "temperature": 0.7, "top_p": 1, "suffix": "", "frequency_penalty": 0.0, "presence_penalty": 0.0, "logprobs": 1, "n": 1, "echo": false, "stop": null}, "openai2": {"platform": "openai", "model_name": "text-curie-001", "max_tokens": 100, "temperature": 0, "top_p": 1, "suffix": "", "frequency_penalty": 0, "presence_penalty": 0, "logprobs": 1, "n": 1, "echo": false, "stop": null}}, "software_configs": {"summarization_general": {"name": "summarization_general", "description": "Translates difficult text into simpler concepts.", "instruction": "Summarize the following text into simpler concepts:", "demonstration": [], "prompt_template": "lambda input: f\"{input['text']}\\n\",", "task": "summarization"}}, "dataset_configs": null, "example": {"input": {"text": "Jupiter is the fifth planet from the Sun and the largest in the Solar System. It is a gas giant with a mass one-thousandth that of the Sun, but two-and-a-half times that of all the other planets in the Solar System combined. Jupiter is one of the brightest objects visible to the naked eye in the night sky, and has been known to ancient civilizations since before recorded history. It is named after the Roman god Jupiter.[19] When viewed from Earth, Jupiter can be bright enough for its reflected light to cast visible shadows,[20] and is on average the third-brightest natural object in the night sky after the Moon and Venus."}, "output": "Jupiter is a gas giant planet located fifth from the Sun, and is the largest planet in the solar system. It has a mass one thousandth that of the sun and is two and a half times that of all the other planets in the solar system combined. Jupiter is one of the brightest objects visible in the night sky, and has been known to ancient civilizations for centuries. It is named after the Roman god Jupiter. When viewed from Earth, it can cast shadows with its reflected light."}, "example_prompt": "Summarize the following text into simpler concepts:\n\nJupiter is the fifth planet from the Sun and the largest in the Solar System. It is a gas giant with a mass one-thousandth that of the Sun, but two-and-a-half times that of all the other planets in the Solar System combined. Jupiter is one of the brightest objects visible to the naked eye in the night sky, and has been known to ancient civilizations since before recorded history. It is named after the Roman god Jupiter.[19] When viewed from Earth, Jupiter can be bright enough for its reflected light to cast visible shadows,[20] and is on average the third-brightest natural object in the night sky after the Moon and Venus.\n\n"}}
{"text_to_command---default": {"description": "This promptware is used to convert the text to a programmatic command.", "creator": "OpenAI", "homepage": "https://beta.openai.com/examples/", "reference": "", "codebase_url": "https://beta.openai.com/examples/", "license": "no-license", "task": null, "research_tasks": ["conditional-generation"], "application_categories": ["transformation"], "application_subcategories": ["translation"], "original_platform": "gpt3", "design_pattern": "standalone", "source_language": "en", "target_language": "en", "module_name": "text_to_command_promptware", "config_name": "default", "kernel_configs": {"openai": {"platform": "openai", "model_name": "text-curie-001", "max_tokens": 64, "temperature": 0, "top_p": 1, "suffix": "", "frequency_penalty": 0.0, "presence_penalty": 0.0, "logprobs": 1, "n": 1, "echo": false, "stop": null}}, "software_configs": {"text_to_command": {"name": "text_to_command", "description": "This promptware is used to convert the text to a programmatic command.", "instruction": "Convert this text to a programmatic command:", "demonstration": ["Ask Constance if we need some bread\nsend-msg `find constance` Do we need some bread?\n\n"], "prompt_template": "lambda input: f\"{input['text']}\",", "task": "conditional-generation"}}, "dataset_configs": null, "example": {"input": {"text": "Reach out to the ski store and figure out if I can get my skis fixed before I leave on Thursday\n"}, "output": "send-msg `find ski store` Can you tell me if you have any skis in stock that I can use before I leave on Thursday?"}, "example_prompt": "Convert this text to a programmatic command:\nAsk Constance if we need some bread\nsend-msg `find constance` Do we need some bread?\n\n\nReach out to the ski store and figure out if I can get my skis fixed before I leave on Thursday\n\n"}}
{"third_person_converter---default": {"description": "This promptware is used to converts first-person POV to the third-person.", "creator": "OpenAI", "homepage": "https://beta.openai.com/examples/", "reference": "", "codebase_url": "https://beta.openai.com/examples/", "license": "no-license", "task": null, "research_tasks": ["conditional-generation"], "application_categories": ["transformation"], "application_subcategories": ["rewriting"], "original_platform": "gpt3", "design_pattern": "standalone", "source_language": "en", "target_language": "en", "module_name": "third_person_converter_promptware", "config_name": "default", "kernel_configs": {"openai": {"platform": "openai", "model_name": "text-davinci-003", "max_tokens": 60, "temperature": 0, "top_p": 1.0, "suffix": "", "frequency_penalty": 0.0, "presence_penalty": 0.0, "logprobs": 1, "n": 1, "echo": false, "stop": null}}, "software_configs": {"third_person_converter": {"name": "third_person_converter", "description": "This promptware is used to converts first-person POV to the third-person. This is modified from a community prompt to use fewer examples.", "instruction": "Convert this from first-person to third person (gender female):\n\n", "demonstration": [], "prompt_template": "lambda input: f\"{input['text']}\",", "task": "conditional-generation"}}, "dataset_configs": null, "example": {"input": {"text": "I decided to make a movie about Ada Lovelace."}, "output": "She decided to make a movie about Ada Lovelace."}, "example_prompt": "Convert this from first-person to third person (gender female):\n\n\n\nI decided to make a movie about Ada Lovelace.\n"}}
{"translate_programming_languages---default": {"description": "This promptware is used to translate from one programming language to another we can use the comments to specify the source and target languages.", "creator": "OpenAI", "homepage": "https://beta.openai.com/examples/", "reference": "", "codebase_url": "https://beta.openai.com/examples/", "license": "no-license", "task": null, "research_tasks": ["conditional-generation"], "application_categories": ["transformation"], "application_subcategories": ["translation"], "original_platform": "gpt3", "design_pattern": "standalone", "source_language": "python", "target_language": "haskell", "module_name": "translate_programming_languages_promptware", "config_name": "default", "kernel_configs": {"openai": {"platform": "openai", "model_name": "code-davinci-002", "max_tokens": 54, "temperature": 0, "top_p": 1.0, "suffix": "", "frequency_penalty": 0.0, "presence_penalty": 0.0, "logprobs": 1, "n": 1, "echo": false, "stop": ["###"]}}, "software_configs": {"translate_programming_languages": {"name": "translate_programming_languages", "description": "This promptware is used to translate from one programming language to another we can use the comments to specify the source and target languages.", "instruction": "", "demonstration": [], "prompt_template": "lambda input: f\"{input['text']}\",", "task": "conditional-generation"}}, "dataset_configs": null, "example": {"input": {"text": "##### Translate this function from Python into Haskell\n### Python\n    \n    def predict_proba(X: Iterable[str]):\n        return np.array([predict_one_probas(tweet) for tweet in X])\n    \n### Haskell"}, "output": "```haskell\npredictProba :: [String] -> [Double]\npredictProba xs = map predictOneProbas xs\n```"}, "example_prompt": "\n##### Translate this function from Python into Haskell\n### Python\n    \n    def predict_proba(X: Iterable[str]):\n        return np.array([predict_one_probas(tweet) for tweet in X])\n    \n### Haskell\n"}}
{"turn_by_turn_directions---default": {"description": "This promptware is used to convert natural language to turn-by-turn directions.", "creator": "OpenAI", "homepage": "https://beta.openai.com/examples/", "reference": "", "codebase_url": "https://beta.openai.com/examples/", "license": "no-license", "task": null, "research_tasks": ["conditional-generation"], "application_categories": ["transformation"], "application_subcategories": ["rewriting"], "original_platform": "gpt3", "design_pattern": "standalone", "source_language": "en", "target_language": "en", "module_name": "turn_by_turn_directions_promptware", "config_name": "default", "kernel_configs": {"openai": {"platform": "openai", "model_name": "text-davinci-003", "max_tokens": 64, "temperature": 0.3, "top_p": 1.0, "suffix": "", "frequency_penalty": 0.0, "presence_penalty": 0.0, "logprobs": 1, "n": 1, "echo": false, "stop": null}}, "software_configs": {"turn_by_turn_directions": {"name": "turn_by_turn_directions", "description": "This promptware is used to convert natural language to turn-by-turn directions.", "instruction": "Create a numbered list of turn-by-turn directions from this text: \n\n", "demonstration": [], "prompt_template": "lambda input: f\"{input['text']}\",", "task": "conditional-generation"}}, "dataset_configs": null, "example": {"input": {"text": "Go south on 95 until you hit Sunrise boulevard then take it east to us 1 and head south. Tom Jenkins bbq will be on the left after several miles."}, "output": "1. Go south on 95\n2. Take Sunrise Boulevard east\n3. Head south on US 1\n4. Tom Jenkins BBQ will be on the left after several miles"}, "example_prompt": "Create a numbered list of turn-by-turn directions from this text: \n\n\n\nGo south on 95 until you hit Sunrise boulevard then take it east to us 1 and head south. Tom Jenkins bbq will be on the left after several miles.\n"}}
{"vr_fitness_idea_generator---default": {"description": "This promptware is used to create ideas for fitness and virtual reality games.", "creator": "OpenAI", "homepage": "https://beta.openai.com/examples/", "reference": "", "codebase_url": "https://beta.openai.com/examples/", "license": "no-license", "task": null, "research_tasks": ["conditional-generation"], "application_categories": ["brainstorming"], "application_subcategories": ["advice-giving"], "original_platform": "gpt3", "design_pattern": "standalone", "source_language": "en", "target_language": "en", "module_name": "vr_fitness_idea_generatore_promptware", "config_name": "default", "kernel_configs": {"openai": {"platform": "openai", "model_name": "text-davinci-003", "max_tokens": 150, "temperature": 0.6, "top_p": 1.0, "suffix": "", "frequency_penalty": 1, "presence_penalty": 1, "logprobs": 1, "n": 1, "echo": false, "stop": null}}, "software_configs": {"vr_fitness_idea_generator": {"name": "vr_fitness_idea_generator", "description": "This promptware is used to create ideas for fitness and virtual reality games.", "instruction": "", "demonstration": [], "prompt_template": "lambda input: f\"{input['text']}\",", "task": "conditional-generation"}}, "dataset_configs": null, "example": {"input": {"text": "Brainstorm some ideas combining VR and fitness:"}, "output": "1. Virtual reality exercise classes \n2. Virtual reality running and cycling simulations \n3. Interactive virtual fitness challenges \n4. Immersive yoga experiences in a virtual environment \n5. Virtual strength training with personalized feedback from trainers  \n6. VR-based aerobic workouts for home use  \n7. Gamified weightlifting activities using motion tracking controllers  \n8. Tracking progress of physical activity through 3D avatars"}, "example_prompt": "\nBrainstorm some ideas combining VR and fitness:\n"}}
{"write_a_python_docstring---default": {"description": "This promptware is used to create a docstring for a given Python function. We specify the Python version, paste in the code, and then ask within a comment for a docstring, and give a characteristic beginning of a docstring (\"\"\").", "creator": "OpenAI", "homepage": "https://beta.openai.com/examples/", "reference": "", "codebase_url": "https://beta.openai.com/examples/", "license": "no-license", "task": null, "research_tasks": ["conditional-generation"], "application_categories": ["transformation"], "application_subcategories": ["explanation"], "original_platform": "gpt3", "design_pattern": "standalone", "source_language": "python", "target_language": "en", "module_name": "write_a_python_docstring_promptware", "config_name": "default", "kernel_configs": {"openai": {"platform": "openai", "model_name": "text-davinci-003", "max_tokens": 150, "temperature": 0, "top_p": 1.0, "suffix": "", "frequency_penalty": 0.0, "presence_penalty": 0.0, "logprobs": 1, "n": 1, "echo": false, "stop": ["#", "\"\"\""]}}, "software_configs": {"write_a_python_docstring": {"name": "write_a_python_docstring", "description": "This promptware is used to create a docstring for a given Python function. We specify the Python version, paste in the code, and then ask within a comment for a docstring, and give a characteristic beginning of a docstring (\"\"\").", "instruction": "", "demonstration": [], "prompt_template": "lambda input: f\"{input['text']}\",", "task": "conditional-generation"}}, "dataset_configs": null, "example": {"input": {"text": "# Python 3.7\n \ndef randomly_split_dataset(folder, filename, split_ratio=[0.8, 0.2]):\n    df = pd.read_json(folder + filename, lines=True)\n    train_name, test_name = \"train.jsonl\", \"test.jsonl\"\n    df_train, df_test = train_test_split(df, test_size=split_ratio[1], random_state=42)\n    df_train.to_json(folder + train_name, orient='records', lines=True)\n    df_test.to_json(folder + test_name, orient='records', lines=True)\nrandomly_split_dataset('finetune_data/', 'dataset.jsonl')\n    \n# An elaborate, high quality docstring for the above function:\n\"\"\""}, "output": "This function randomly splits a dataset into two parts, a training set and a test set, and saves them as separate files.\n\nParameters:\n    folder (str): The path to the folder containing the dataset file.\n    filename (str): The name of the dataset file.\n    split_ratio (list): A list of two floats representing the ratio of the training set and the test set.\n\nReturns:\n    None"}, "example_prompt": "\n# Python 3.7\n \ndef randomly_split_dataset(folder, filename, split_ratio=[0.8, 0.2]):\n    df = pd.read_json(folder + filename, lines=True)\n    train_name, test_name = \"train.jsonl\", \"test.jsonl\"\n    df_train, df_test = train_test_split(df, test_size=split_ratio[1], random_state=42)\n    df_train.to_json(folder + train_name, orient='records', lines=True)\n    df_test.to_json(folder + test_name, orient='records', lines=True)\nrandomly_split_dataset('finetune_data/', 'dataset.jsonl')\n    \n# An elaborate, high quality docstring for the above function:\n\"\"\"\n"}}
